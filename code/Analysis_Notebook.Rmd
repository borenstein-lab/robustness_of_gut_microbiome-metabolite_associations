---
title: "Analysis Notebook: A meta-analysis study of the robustness of gut microbiome-metabolome associations"
output:
  html_document:
    css: Custom_Formatting.css
    toc: true
    toc_depth: 3
    df_print: paged
---

## 0. Preparations

R version: 3.6.1 (2019-07-05)

```{r PREPARATIONS, message=FALSE}
# Machine learning packages
library(ranger) # Fast implementation of Random Forests, Version 0.12.1
library(compositions) # Version 1.40-5
library(tidymodels) # Version 0.1.0  

# Files I/O packages
library(gdata) # Version 2.18.0
library(readr) # Version 1.3.1

# Data visualization packages
library(RColorBrewer) # Version 1.1-2
library(viridis) # Version 0.5.1 
library(ggplot2) # Version 3.2.1 
library(cowplot) # Version 1.0.0
library(kableExtra) # Version 1.1.0
library(grid) # Version 3.6.1
library(gridExtra) # Version 2.3 
library(ggsignif) # Version 0.6.0 
library(pheatmap) # Version 1.0.12
library(dendextend) # Version 1.12.0
library(corrgram) # Version 1.13 

# Misc. packages / utilities
library(tictoc) # Track runtimes, Version 1.0 
library(DescTools) # FisherZInv function, Version 0.99.31 
library(Hmisc) # Version 4.3-0
library(weights) # Weighted correlation, Version 1.0.1 
library(vegan) # Scaling: decostand, Version 2.5-6 
source("Utilities.R")

# For parallel processing
library(doParallel) # Version 1.0.15 
library(foreach) # Version 1.5.0   
all_cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = all_cores)

# Meta-analysis
library(meta) # Version 4.9-9
library(metafor) # Version 2.1-0
# Sub-group meta-analysis function adapted from from dmetar package
source("Subgroup_Meta-Analysis_Mixed_Effects.R")

options(scipen=999)
ggplot2::theme_set(theme_light())
knitr::opts_chunk$set(fig.align = 'center', cache=TRUE)
```

## 1. Load data

In this section we load the data from an RData file, downsample temporal datasets, and print some statistics.  

The RData file includes:  
* A table with statistics per processed dataset   
* Genera abundances per dataset   
* Metabolite levels per dataset  
* Metabolite mappings/metadata per dataset  
* Subject metadata per dataset  

Notes about data processing are available in the manuscript's supplementary information.

```{r LOAD_DATA}
load("../data/Processed_Data_Backup_29092020.RData")
message("Data loaded")
```

### Downsample temporal datasets

To better leverage the longitudinal datasets without causing biases in our ML models, we randomly choose up to 3 samples per subject, in every temporal dataset we have. This way we assure no single subject has a unproportional effect on the trained model. (Still, for testing we always take 1 sample per subject).

```{r DOWNSAMPLE_TEMPORAL_DATASETS}
# Choose temporal datasets that need downsampling
# Note: I ignore datasets that do not have enough samples to begin with
datasets.summary <- datasets.summary %>%
  mutate(N.samples.original = datasets.summary$N.samples) %>%
  mutate(N.subjects = -1)
datasets.to.downsample <- c("POYET_BIO_ML_HEALTHY",
                            "iHMP_IBD_HEALTHY",
                            "iHMP_IBD_DISEASE",
                            "MARS_IBS_HEALTHY",
                            "MARS_IBS_DISEASE")

# Iterate over datasets
for(dataset in datasets.to.downsample) {
  # Get a reproducible list of samples per subject
  samples.to.keep <- downsample.by.subject(dataset, metadatas, genera.trans)
  # Update data tables (microbiome + metabolome)
  for(tmp.trans in names(genera.trans)) {
    genera.trans[[tmp.trans]][[dataset]] <- 
      genera.trans[[tmp.trans]][[dataset]][,samples.to.keep]
  }
  for(tmp.trans in names(mtb.trans)) {
    mtb.trans[[tmp.trans]][[dataset]] <- 
      mtb.trans[[tmp.trans]][[dataset]][,samples.to.keep]
  }
  # Update table with summary stats per dataset
  datasets.summary$N.samples[datasets.summary$Dataset == dataset] <- 
    length(samples.to.keep)
  datasets.summary$N.subjects[datasets.summary$Dataset == dataset] <- 
    n_distinct(metadatas[[dataset]] %>% 
                 filter(Sample %in% samples.to.keep) %>% 
                 pull(Subject.ID))
}

message("Downsampled temporal datasets")
rm(tmp.trans, dataset, samples.to.keep, datasets.to.downsample)
```

### Choose datasets

```{r}
datasets.to.train.healthy <- c("FRANZOSA_IBD_HEALTHY",
                       "YACHIDA_CRC_HEALTHY", 
                       "KIM_ADENOMAS_HEALTHY", 
                       "JACOBS_IBD_RELATIVES_HEALTHY",
                       "HE_INFANTS_HEALTHY",
                       "SINHA_CRC_HEALTHY",
                       "POYET_BIO_ML_HEALTHY",
                       "iHMP_IBD_HEALTHY",
                       "MARS_IBS_HEALTHY")

datasets.to.train.disease <- c("SINHA_CRC_DISEASE",
                               "KIM_ADENOMAS_DISEASE", 
                               #"HOFFMAN_CF_DISEASE",
                               "YACHIDA_CRC_DISEASE", 
                               "ERAWIJANTARI_GC_DISEASE",
                               "FRANZOSA_IBD_DISEASE",
                               "MARS_IBS_DISEASE",
                               "iHMP_IBD_DISEASE")

datasets.to.train <- 
  c(datasets.to.train.healthy, datasets.to.train.disease)

# Create a mapping from dataset subset name to original dataset name
# (e.g. map both "MARS_IBS_HEALTHY" and "MARS_IBS_DISEASE" to "MARS_IBS")
datasets.map.to.orig <- subset.to.orig.dataset[datasets.to.train]
names(datasets.map.to.orig) <- datasets.to.train
  
print("Working on datasets:")
print(datasets.to.train)
```

### Datasets summary - healthy + disease

```{r}
# ----------------- Organize and print statistics ------------------>
tmp <- datasets.summary %>% 
  filter(Dataset %in% datasets.to.train) 

tmp %>% 
  select(-Dataset.subset, -Dataset.Original, -Raw.Seq.Depth, -N.metabolites, -N.KEGG.metabolites, -N.samples.original, -Longitudinal, -N.genera) %>%
  kable() %>%
  kable_styling()

tmp <- tmp %>%
  mutate(N.subjects2 = ifelse(N.subjects > -1, N.subjects, N.samples))

print(paste("Overall, we use data from", 
            n_distinct(tmp$Dataset.Original),
            "studies, including",
            sum(tmp$N.samples[tmp$Dataset.subset == "Healthy"]), 
            "samples from",
            sum(tmp$N.subjects2[tmp$Dataset.subset == "Healthy"]), "'healthy' subjects, and",
            sum(tmp$N.samples[tmp$Dataset.subset != "Healthy"]), 
            "samples from",
            sum(tmp$N.subjects2[tmp$Dataset.subset != "Healthy"]), "'case' subjects"))

rm(tmp)
```

### Analyze feature overlap in healthy datasets

We analyze the overlap in features across datasets.  
Notably, we only consider features that pass our filters (prevalant over a threshold in the healthy cohort, annotated at genus level / HMDB annotated, etc.). This means that the number of features is also affected by the number of samples in the dataset (and not only technical aspects such as sequencing depth or metabolomics platform).

```{r CHECK_OVERLAP_OF_FEATURES, fig.width=8, fig.height=7.2}
# ------------------- Place holder for results --------------------->
feat.overlap.pairwise <- data.frame(stringsAsFactors = F)

# -------------- Collect stats per pair of datasets ---------------->
for (i in 1:(length(datasets.to.train.healthy)-1)) { #i=1
  for (j in (i+1):length(datasets.to.train.healthy)) { #j=2
    tmp.dataset.1 <- datasets.to.train.healthy[i]
    tmp.dataset.2 <- datasets.to.train.healthy[j]
    
    for (feat.type in c("Microbes", "Metabolites")) { #feat.type = "Microbes"
      
      # Get list of available features per dataset
      if (feat.type == "Microbes") {
        tmp.feat.list.1 <- rownames(genera.trans$RELATIVE[[tmp.dataset.1]])
        tmp.feat.list.2 <- rownames(genera.trans$RELATIVE[[tmp.dataset.2]])
      } else {
        tmp.feat.list.1 <- metab.mappings[[tmp.dataset.1]] %>%
          filter(Compound %in% rownames(mtb.trans$LOG[[tmp.dataset.1]])) %>%
          pull(HMDB) %>%
          unique()
        tmp.feat.list.2 <- metab.mappings[[tmp.dataset.2]] %>%
          filter(Compound %in% rownames(mtb.trans$LOG[[tmp.dataset.2]])) %>%
          pull(HMDB) %>%
          unique()
      }
      
      N.shared <- length(intersect(tmp.feat.list.1, tmp.feat.list.2))
      N.non.shared <- length(setdiff(union(tmp.feat.list.1,tmp.feat.list.2), 
                                     intersect(tmp.feat.list.1,tmp.feat.list.2)))
      
      feat.overlap.pairwise <- 
        bind_rows(feat.overlap.pairwise,
                  data.frame(Dataset.1 = ifelse(feat.type == "Microbes", 
                                                tmp.dataset.1, tmp.dataset.2), 
                             Dataset.2 = ifelse(feat.type == "Microbes", 
                                                tmp.dataset.2, tmp.dataset.1), 
                             Feature.Type = feat.type,
                             Comparison.Category = 
                               c(paste(feat.type,"shared",sep=": "),
                                                                          
                                 paste(feat.type,"non-shared",sep=": ")),
                             Count = c(N.shared, N.non.shared)))
    }
  }
}

feat.overlap.pairwise <- feat.overlap.pairwise %>%
  mutate(Dataset.1 = factor(Dataset.1, levels = datasets.to.train.healthy,
                            labels = substr(datasets.to.train.healthy, 1, 2))) %>%
  mutate(Dataset.2 = factor(Dataset.2, levels = datasets.to.train.healthy,
                            labels = substr(datasets.to.train.healthy, 1, 2)))

# ----------------------------- Plot ------------------------------->
ggplot(feat.overlap.pairwise, 
       aes(fill=Comparison.Category, y=Count, x=1.5)) + 
  geom_bar(position="fill", stat="identity", color="black") +
  facet_grid(Dataset.2 ~ Dataset.1, switch = "both") +
  coord_polar(theta="y", start = 0) +
  xlim(0.5, 2) +
  theme_bw() +
  ylab(NULL) +
  xlab(NULL) +
  ggtitle("How much do different datasets overlap in terms of \n available genera / metabolites?") +
  scale_fill_manual(values = c("Microbes: non-shared" = "darkseagreen2",
                               "Microbes: shared" = "darkgreen",
                               "Metabolites: non-shared" = "azure2",
                               "Metabolites: shared" = "dodgerblue4")) +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.grid  = element_blank(),
        panel.spacing = unit(0, "lines"),
        plot.title = element_text(hjust = 0.5),
        legend.title = element_blank(),
        strip.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 10),
        strip.text.y = element_text(angle = 180, hjust = 1, size = 10),
        strip.background = element_blank(),
        panel.border = element_blank())

rm(i,j,feat.type,tmp.dataset.1,tmp.dataset.2,
   tmp.feat.list.1,tmp.feat.list.2,N.shared,N.non.shared)
```


## 2. Analysis settings

In this section we choose the specific metabolites to be analyzed and specific settings for the machine learning pipeline. We additionally print statistics regarding the prevalance of different metabolites and different genera among datasets included in the analysis.  

### Choose metabolites

Get list of HMDB ID's that are common to at least 3 of the above (distinct) datasets.  
We also record exactly which HMDB ID's appears in each dataset. 

```{r fig.width=4.5, fig.height=3}
# ------ Compose a list of unique HMDB IDs from all datasets ------->
# Initialize an empty table with column per dataset 
metabolites.per.dataset <- data.frame(stringsAsFactors = F)

for (dataset in datasets.to.train) {
  # Fetch the dataset's metabolite mapping file
  tmp <- metab.mappings[[dataset]]
  
  # Get all HMDB ID's in the dataset (after our pre-processing)
  hmdb.metabs.in.dataset <- 
    tmp$HMDB[as.character(tmp$Compound) %in% 
               rownames(mtb.trans$NONE[[dataset]])]
  
  # Remove dups (metabolites may appear more than once)
  hmdb.metabs.in.dataset <- unique(hmdb.metabs.in.dataset)
  
  # Add HMDBs to our table (if not in there already)
  new.hmdb.metabs <- 
    hmdb.metabs.in.dataset[! hmdb.metabs.in.dataset %in% 
                             metabolites.per.dataset$HMDB]
  
  new.hmdb.metabs <- data.frame(HMDB = new.hmdb.metabs, stringsAsFactors = F)
  metabolites.per.dataset <- bind_rows(metabolites.per.dataset, new.hmdb.metabs)
  
  # Mark that they appear in the current dataset
  metabolites.per.dataset <- metabolites.per.dataset %>%
    mutate(!!dataset := ifelse(HMDB %in% hmdb.metabs.in.dataset,
                               TRUE, FALSE))
}

# Replace all NAs with FALSE
metabolites.per.dataset[is.na(metabolites.per.dataset)] <- FALSE

# Count for each metabolite how many datasets have it
metabolites.per.dataset <- metabolites.per.dataset %>%
  mutate(N.Datasets.Total = rowSums(.[-1])) %>%
  mutate(N.Datasets.Healthy = rowSums(.[datasets.to.train.healthy])) %>%
  mutate(N.Datasets.Distinct = -1)  # Place holder

# Fill "N.Datasets.Distinct" column
for(i in 1:nrow(metabolites.per.dataset)) {
  tmp <- data.frame(orig.dataset = subset.to.orig.dataset[datasets.to.train],
                    flag = unlist(metabolites.per.dataset[i, datasets.to.train]))
  tmp <- tmp %>%
    group_by(orig.dataset) %>%
    summarise(flag.orig.dataset = max(flag), .groups = "drop")
  metabolites.per.dataset[i, "N.Datasets.Distinct"] <- sum(tmp$flag.orig.dataset)
}

# ---------------------- Print/plot statistics --------------------->
print(paste(nrow(metabolites.per.dataset),
            "unique HMDB compound IDs were found across all datasets"))

print(paste(nrow(metabolites.per.dataset %>% filter(N.Datasets.Healthy > 0)),
            "unique HMDB compound IDs were found across all *healthy* datasets"))

tmp <- metabolites.per.dataset %>%
  filter(N.Datasets.Healthy > 0) %>%
  group_by(N.Datasets.Healthy) %>%
  summarise(N = n(), .groups = "drop") %>%
  arrange(-N.Datasets.Healthy) %>%
  mutate(cum.N = cumsum(N))

ggplot(tmp, aes(x = N.Datasets.Healthy, y = cum.N)) +
  geom_bar(stat="identity", color='black',fill='azure2') +
  theme_classic() +
  ggtitle("How common are HMDB metabolites among (healthy) datasets?") +
  ylab("No. of HMDB metabolites") +
  xlab("No. of datasets") +
  scale_x_continuous(breaks=c(2,4,6,8,10,12,14)) +
  geom_text(aes(label=cum.N), vjust = -0.5, size = 3) +
  #geom_text(aes(label=paste("N =",cum.N)), vjust = -0.5, size = 2.5) +
  scale_y_continuous(limits = c(0, max(tmp$cum.N) + 20)) +
  theme(plot.title = element_text(hjust = 0.5, size = 10), 
        axis.text.y = element_text(size = 7),
        axis.title = element_text(size = 10))

# Get a list of common HMDBs (those that appear in >=3 datasets)
common.HMDBs <- metabolites.per.dataset %>%
  filter(N.Datasets.Distinct >= 3) %>%
  pull(HMDB)
print(paste(length(common.HMDBs), "metabolites appear in >= 3 distinct datasets"))

common.HMDBs.healthy <- metabolites.per.dataset %>%
  filter(N.Datasets.Healthy >= 3) %>%
  pull(HMDB)
print(paste(length(common.HMDBs.healthy), "metabolites appear in >= 3 healthy datasets"))

# ---------------------------- Clean up ---------------------------->
rm(tmp, hmdb.metabs.in.dataset, dataset, new.hmdb.metabs, i)
```

### Annotate low-confidence metabolites 

Add an annotation for metabolites that were not identified with high confidence in one or more of the studies. See supplementary information for further details.

```{r}
# Initialize tables to store relevant statistics
metabolites.conf.flags <- data.frame(HMDB = common.HMDBs, 
                                     Low.Confidence.Count = 0,
                                     stringsAsFactors = F)
metabolites.conf.flags.per.ds <- data.frame(Dataset = character(0), 
                                      Compound = character(0), 
                                      High.Confidence = logical(0), 
                                      stringsAsFactors = F)

# Iterate over datasets (original ones included in any of the analyses)
for (dataset in unique(datasets.map.to.orig)) { 
  tmp <- metab.mappings[[dataset]]
  tmp <- tmp[! tmp$High.Confidence, ]
  
  # Update tables
  metabolites.conf.flags <- metabolites.conf.flags %>%
    mutate(Low.Confidence.Count = ifelse(HMDB %in% tmp$HMDB, 
                                         Low.Confidence.Count+1, 
                                         Low.Confidence.Count))
  
  metabolites.conf.flags.per.ds <- bind_rows(metabolites.conf.flags.per.ds,
                                             tmp %>%
                                               select(Compound, High.Confidence) %>%
                                               mutate(Dataset = dataset))
}

print(paste("Out of the", 
            nrow(metabolites.conf.flags),
            "metabolites shared across 3 or more datasets,",
            sum(metabolites.conf.flags$Low.Confidence.Count>0),
            "metabolites are annotated with lower confidence flag"))

rm(dataset, tmp)
```

### Analyze shared genera features

```{r fig.width=4.5, fig.height=3}
# --------------- Initialize table for genera statistics ----------->
genera.dataset.stats <- data.frame(Taxon = character(0),
                           Dataset = character(0),
                           Dataset.N = integer(0),
                           Taxon.Mean.Abundance = numeric(0),
                           Taxon.Var.Abundance = numeric(0),
                           Taxon.Perc.of.Non.Zeros = numeric(0),
                           stringsAsFactors = FALSE)

# We iterate over each dataset and record a few basic stats per each genus in each dataset
for (dataset in datasets.to.train.healthy) {
  tmp.genera <- rownames(genera.trans$RELATIVE[[dataset]])
  tmp.means <- unname(apply(genera.trans$RELATIVE[[dataset]], MARGIN = 1, mean))
  tmp.vars <- unname(apply(genera.trans$RELATIVE[[dataset]], MARGIN = 1, var))
  tmp.non.zero.perc <- unname(apply(genera.trans$RELATIVE[[dataset]], 
                                    MARGIN = 1, 
                                    function(r) {100*sum(r>0)/length(r)}))
  tmp <- data.frame(Taxon = tmp.genera, 
                    Dataset = dataset,
                    Dataset.N = ncol(genera.trans$RELATIVE[[dataset]]),
                    Taxon.Mean.Abundance = tmp.means,
                    Taxon.Var.Abundance = tmp.vars,
                    Taxon.Perc.of.Non.Zeros = tmp.non.zero.perc,
                    stringsAsFactors = FALSE)
  genera.dataset.stats <- bind_rows(genera.dataset.stats, tmp)
}

# We further summarize these stats to get basic statistics at the genus level (average over datasets)
genera.stats <- genera.dataset.stats %>%
  group_by(Taxon) %>%
  summarise(Taxon.Overall.Mean.Abundance = weighted.mean(x = Taxon.Mean.Abundance, 
                                                         w = Dataset.N),
            Taxon.Overall.Perc.Non.Zero = weighted.mean(x = Taxon.Perc.of.Non.Zeros, 
                                                        w = Dataset.N),
            N.Datasets.Including.Taxon = n(),
            .groups = "drop") %>%
  mutate(Genus.Only = gsub(".*\\|g__","g__", Taxon))

# ---------------------- Print/plot statistics --------------------->
print(paste(nrow(genera.stats),
            "unique genera were found across all healthy datasets"))

tmp <- genera.stats %>%
  group_by(N.Datasets.Including.Taxon) %>%
  summarise(N = n(), .groups = "drop") %>%
  arrange(-N.Datasets.Including.Taxon) %>%
  mutate(cum.N = cumsum(N))

ggplot(tmp, aes(x = N.Datasets.Including.Taxon, y = cum.N)) +
  geom_bar(stat="identity", color='black', fill='azure2') +
  theme_classic() +
  ggtitle("How common are genera among datasets?") +
  ylab("No. of genera") +
  xlab("No. of datasets") +
  scale_x_continuous(breaks=c(2,4,6,8,10,12,14)) +
  #geom_text(aes(label=paste("N =",cum.N)), vjust = -0.5, size = 2.5) +
  geom_text(aes(label=cum.N), vjust = -0.5, size = 3) +
  scale_y_continuous(limits = c(0, max(tmp$cum.N) + 7)) +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.title = element_text(size = 10))

# ---------------------------- Clean up ---------------------------->
rm(dataset, tmp.genera, tmp.means, tmp.vars, tmp.non.zero.perc, tmp)
```


### Choose ML pipeline options

LOOCV = Leave one out cross validation, LOSO = Leave one subject out.  

```{r}
param.tuning <- FALSE
num.runs <- 5
model.options <- c("RF") #c("ENet", "RF")
genera.trans.options <- c("RELATIVE") #,"CLR")
tuning.cv.k <- 10 # Folds for (inner) CV
test.cv.k <- 0 # 0 for LOOCV / LOSO-CV # TODO: no support for >0 numbers at the moment (following LOSO support)

message("Number of runs per task: ", num.runs)
message("Model options: ", paste(model.options, collapse = ", "))
message("Genera transformation options: ", paste(genera.trans.options, collapse = ", "))
message("K for tuning cross-validation: ", tuning.cv.k)
message("K for testing cross-validation (0 = LOOCV): ", test.cv.k)
```

## 3. Train regression machine learning models

At this point we save an image with all R data thus far, to be used with the seperate R script that runs the entire machine learning pipeline (Machine_Learning_Pipeline_For_Condor.R). 

```{r, eval = FALSE}
save.image(file='data/Image_For_Condor.RData')
```

We then run the script with various settings (see documentation in script) and for each of the datasets. Finally all results are merged into a single table which we now load back into the main notebook.  
Each line in the table represents a specific set of pipeline settings, a specific metabolite and a specific dataset. Furthermore, we execute each such task 5 times to also estimate results stability.   

```{r ORGANIZE_RESULTS, message = FALSE}
# Read if not exists already
metab.pred.results <- read_delim("../data/Combined_Metabolome_Predictability_Results.tsv", 
                                   "\t", escape_double = FALSE, trim_ws = TRUE)
metab.pred.results <- metab.pred.results %>% 
    filter(PipeSettings.Genera.Trans == "RELATIVE")

# If results include datasets not selected above, remove
metab.pred.results <- metab.pred.results %>%
  filter(Task.Dataset %in% datasets.to.train) %>%
  filter(Task.HMDB %in% common.HMDBs)

# For metabolites that appeared more than once in a specific dataset (i.e. same HMDB appears more than once),
#  we take only the raw-metabolite with best predictability (averaged over 5 runs).
keep.these <- metab.pred.results %>%
  # Group by task
  group_by(Task.Dataset, 
           Task.HMDB, 
           Task.Metabolite.Raw, 
           PipeSettings.Genera.Trans, 
           PipeSettings.ML.Model, 
           PipeSettings.Shuffled, 
           PipeSettings.Tuning) %>%
  # Get average performance
  summarise(Avg.Spearman = mean(Test.CV.Spearman.rho), 
            N = n(), .groups = "drop") %>%
  # Group by dataset+HMDB
  group_by(Task.Dataset, 
           Task.HMDB, 
           PipeSettings.Genera.Trans, 
           PipeSettings.ML.Model, 
           PipeSettings.Shuffled, 
           PipeSettings.Tuning) %>%
  # Take only best metabolite
  slice(which.max(Avg.Spearman)) %>%
  # Identifiers of which tasks to keep in the analysis
  select(Task.Dataset, 
         Task.HMDB,
         Task.Metabolite.Raw, 
         PipeSettings.Genera.Trans, 
         PipeSettings.ML.Model, 
         PipeSettings.Shuffled, 
         PipeSettings.Tuning)
  
metab.pred.results <- metab.pred.results %>%
  inner_join(keep.these)

# Add HMDB-unified metabolite name 
metab.pred.results <- metab.pred.results %>%
  left_join(metabolite.stats %>% 
              select(HMDB, Compound.Name), 
            by = c("Task.HMDB" = "HMDB")) 

rm(keep.these)
```


Now we create an additional summarized predictability table, with one row per pipeline setting + prediction task (i.e. dataset + hmdb), i.e. we average results over independent runs.  

```{r}
# Lastly, create a summarized version where metrics are averaged over runs
metab.pred.results.sum <- metab.pred.results %>%
  group_by(Task.Dataset, 
           Task.Metabolite.Raw, 
           Task.HMDB, 
           Compound.Name, 
           PipeSettings.Genera.Trans, 
           PipeSettings.ML.Model, 
           PipeSettings.Tuning, 
           PipeSettings.Shuffled, 
           N.Genera.Features, 
           N.Train.Samples) %>%
  summarise(Avg.Spearman = mean(Test.CV.Spearman.rho),
            Var.Spearman = var(Test.CV.Spearman.rho),
            Avg.Spearman.p = mean(Test.CV.Spearman.p),
            Var.Spearman.p = var(Test.CV.Spearman.p),
            Avg.Rsq = mean(Test.CV.RSq),
            Var.Rsq = var(Test.CV.RSq),
            .groups = 'drop') %>%
  # Add full pipeline identifier
  mutate(Pipe.ID = paste0("Trans'=", 
                          PipeSettings.Genera.Trans,
                          "~Model=",
                          PipeSettings.ML.Model, 
                          "~Tuning=",
                          PipeSettings.Tuning, 
                          "~Shuff'=",
                          PipeSettings.Shuffled)) %>%
  group_by(Pipe.ID, 
           PipeSettings.Genera.Trans, 
           PipeSettings.ML.Model, 
           PipeSettings.Tuning, 
           PipeSettings.Shuffled, 
           Task.Dataset) %>%
  mutate(Avg.Spearman.FDR = p.adjust(Avg.Spearman.p, method = "fdr")) %>%
  mutate(Healthy = grepl("HEALTHY", Task.Dataset)) %>%
  ungroup()
```

## 4. Compare pipelines  

In this section we compare 4 different machine learning pipelines, in terms of how many well-performing models have they produced and in terms of stability over independent runs with the exact same settings.  
For consistency with downstream analysis, we only look at the metabolite in the common.HMDBs.healthy list (even though we actually have additional predictors for any common metabolite, considering disease datasets as well).  
```{r PIPELINE_COMPARISON_PREPS}
# For plotting - pretty names for pipeline settings
map.pipe.names <- c("Trans'=RELATIVE~Model=ENet~Tuning=FALSE~Shuff'=FALSE" = 
                      "Enet without tuning",
                    "Trans'=RELATIVE~Model=ENet~Tuning=TRUE~Shuff'=FALSE" = 
                      "ENet with tuning",
                    "Trans'=RELATIVE~Model=RF~Tuning=FALSE~Shuff'=FALSE" = 
                      "RF without tuning",
                    "Trans'=RELATIVE~Model=RF~Tuning=TRUE~Shuff'=FALSE" = 
                      "RF with tuning",
                    "Trans'=RELATIVE~Model=ENet~Tuning=FALSE~Shuff'=TRUE" = 
                      "Enet without tuning - SHUFFLED",
                    "Trans'=RELATIVE~Model=ENet~Tuning=TRUE~Shuff'=TRUE" = 
                      "ENet with tuning - SHUFFLED",
                    "Trans'=RELATIVE~Model=RF~Tuning=FALSE~Shuff'=TRUE" = 
                      "RF without tuning - SHUFFLED",
                    "Trans'=RELATIVE~Model=RF~Tuning=TRUE~Shuff'=TRUE" = 
                      "RF with tuning - SHUFFLED")

pipe.colors <- c("Enet without tuning" = "#723d46",
                 "ENet with tuning" = "#e26d5c",
                 "RF without tuning" = "#c9cba3",
                 "RF with tuning" = "#ffe1a8")
```

### Number of well predicted metabolites

Per dataset, which method results in the highest number of well-predicted metabolites? (see manuscript for 'well-predicted metabolite' definition)


```{r WP_DEFINITION}
rho.threshold <- 0.3
fdr.threshold <- 0.1
message("Rho threshold: ", rho.threshold)
message("FDR value threshold: ", fdr.threshold)
```


```{r COMPARE_PIPES1, fig.height=6, fig.width=9.5}
tmp <- metab.pred.results.sum %>%
  filter(! PipeSettings.Shuffled) %>%
  filter(Task.HMDB %in% common.HMDBs.healthy) %>%
  group_by(Pipe.ID, Task.Dataset) %>%
  summarise(N.metabolites = n(),
            Spearman.above.0.3 = sum(Avg.Spearman > rho.threshold),
            Spearman.above.0.3.FDR.below.0.1 = sum(Avg.Spearman > rho.threshold & Avg.Spearman.FDR < fdr.threshold),
            .groups = "drop") %>%
  mutate(Percent.well.predicted = Spearman.above.0.3.FDR.below.0.1 * 100 / N.metabolites) %>%
  mutate(Pretty.Pipe.ID = map.pipe.names[Pipe.ID]) %>%
  arrange(Task.Dataset)

ggplot(tmp, aes(x = Pretty.Pipe.ID, fill = Pretty.Pipe.ID, y = Percent.well.predicted)) +
  geom_bar(stat = "identity", color = "black") +
  ylab("% of well-predicted metabolites in dataset") +
  xlab(NULL) +
  coord_flip() +
  scale_fill_manual(values = pipe.colors) +
  theme_classic() +
  facet_wrap(~ Task.Dataset, nrow = 4) +
  theme(legend.position = "none",
        axis.text.y = element_text(size = 9), 
        strip.text = element_text(size = 7.5, hjust = 0))

rm(tmp)
```

Further summing over (healthy) datasets: which method results in most well-predicted-at-least-once?    
We print statistics per pipeline.  

```{r COMPARE_PIPES2}
metab.pred.results.sum %>%
  filter(Healthy) %>%
  filter(Task.HMDB %in% common.HMDBs.healthy) %>%
  group_by(Pipe.ID, Task.HMDB) %>%
  summarise(N.datasets = n(),
            Spearman.above.0.3.P.below.0.1 = 
              sum(Avg.Spearman > rho.threshold & Avg.Spearman.FDR < fdr.threshold), .groups = "drop") %>%
  mutate(Pipe.ID.2 = map.pipe.names[Pipe.ID]) %>%
  group_by(Pipe.ID.2) %>%
  summarise(Never.well.predicted = sum(Spearman.above.0.3.P.below.0.1 == 0),
            Well.predicted.once.or.more = sum(Spearman.above.0.3.P.below.0.1 >= 1), 
            .groups = "drop") %>%
  kable() %>%
  kable_styling()
```

```{r COMPARE_PIPES3}
metab.pred.results.sum %>%
  filter(Healthy) %>%
  filter(Task.HMDB %in% common.HMDBs.healthy) %>%
  mutate(Pipe.ID.2 = map.pipe.names[Pipe.ID]) %>%
  group_by(Pipe.ID.2) %>%
  summarise(N.tasks = n(),
            Spearman.above.0.3.P.below.0.1 = 
              sum(Avg.Spearman > rho.threshold & Avg.Spearman.FDR < fdr.threshold), 
            .groups = "drop") %>%
  kable() %>%
  kable_styling()
```

### Pipeline stability & predictive power

Here we examine (1) pipeline stability, i.e. how stable are results over independent runs (measured by variance in performance metrics), and (2) predictive power, measured by Spearman's correlation.  

Note: not all models have a random element, so if LOOCV is used, no variance should be expected at all.

Here: not limited to common.HMDBs.healthy.

```{r COMPARE_PIPES4, fig.height=3.5, fig.width=5.2}
# Stability comparison using variance in Spearman rho
metab.pred.results.sum %>%
  filter(! PipeSettings.Shuffled) %>%
  mutate(Pretty.Pipe.ID = map.pipe.names[Pipe.ID]) %>%
  ggplot(aes(x = Pretty.Pipe.ID, fill = Pretty.Pipe.ID, y = Var.Spearman)) + 
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white") +
  scale_fill_manual(values = pipe.colors) +
  ylab("Variance in Spearman rho\n---> Higher values = less stable results") + #\n(across independent identical runs)
  xlab(NULL) +
  ggtitle("Comaprison of pipeline \"stability\" -\nSpearman's rho variance") +
  labs(fill = "Pipeline settings") +
  theme(axis.text.x = element_blank())

# Predictive power comparison using average Spearman rho
metab.pred.results.sum %>%
  filter(! PipeSettings.Shuffled) %>%
  mutate(Pretty.Pipe.ID = map.pipe.names[Pipe.ID]) %>%
  ggplot(aes(x = Pretty.Pipe.ID, fill = Pretty.Pipe.ID, y = Avg.Spearman)) + 
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white") +
  scale_fill_manual(values = pipe.colors) +
  ylab("Average Spearman rho\n---> Higher values = more predictive power") + #\n(across independent identical runs)
  xlab(NULL) +
  ggtitle("Comaprison of pipeline \"predictive power\" -\nSpearman's rho") +
  labs(fill = "Pipeline settings") +
  theme(axis.text.x = element_blank())
```

Stability statistics.  
WP = well-predicted.  
Borderline = 1 of 5 runs not well-predicted / 1 of 5 runs well-predicted.  

```{r COMPARE_PIPES5, fig.width=4, fig.height=4}
metab.pred.results %>% 
  # Choose a specific pipeline
  filter(PipeSettings.Genera.Trans == "RELATIVE" &
           (! PipeSettings.Shuffled)) %>% 
  filter(Task.HMDB %in% common.HMDBs.healthy) %>%
  filter(Task.Dataset %in% datasets.to.train.healthy) %>%
  select(PipeSettings.ML.Model, PipeSettings.Tuning, Task.HMDB, Compound.Name, Task.Dataset, Test.CV.Spearman.rho, Test.CV.Spearman.p) %>%
  mutate(Well.predicted.in.run = (Test.CV.Spearman.rho > rho.threshold & Test.CV.Spearman.p < 0.1)) %>%
  group_by(PipeSettings.ML.Model, PipeSettings.Tuning, Task.HMDB, Compound.Name, Task.Dataset) %>%
  summarise(No.WP.of.5 = sum(Well.predicted.in.run), .groups="drop") %>%
  group_by(PipeSettings.ML.Model, PipeSettings.Tuning) %>%
  summarise(N = n(),
            N.always.WP = sum(No.WP.of.5 == 5),
            N.never.WP = sum(No.WP.of.5 == 0),
            N.borderline = sum(No.WP.of.5 %in% c(1,4)),
            .groups="drop") %>%
  mutate(N.full.agreement = N.always.WP+N.never.WP) %>%
  mutate(Perc.always.WP = round(100*N.always.WP/N,2)) %>%
  mutate(Perc.never.WP = round(100*N.never.WP/N,2)) %>%
  mutate(Perc.full.agreement = Perc.always.WP+Perc.never.WP) %>%
  mutate(Perc.full.agree.or.borderline = Perc.full.agreement+round(100*N.borderline/N,2)) %>%
  kable() %>%
  kable_styling()
```

## 5. Predicatability analysis - healthy only

In this section we focus on a single machine learning pipeline, find well-predicted metabolites and compare different datasets.  

### Choose a pipeline 

```{r}
chosen.pipe <- "Trans'=RELATIVE~Model=RF~Tuning=FALSE~Shuff'=FALSE"
chosen.ML.model <- "RF"
chosen.tuning <- FALSE
message("Chose pipeline: ", chosen.pipe)
```

### Compare to shuffled data

Here we want to show that the portion of well-predicted metabolites is not just a data artefact by comparing to results of an identical pipeline but with shuffling of metabolite values per model.

```{r}
tmp <- metab.pred.results.sum %>%
  filter(PipeSettings.ML.Model == chosen.ML.model) %>%
  filter(PipeSettings.Tuning == chosen.tuning) %>%
  filter(Healthy) %>%
  group_by(PipeSettings.Shuffled, Task.Dataset) %>%
  summarise(N.tasks = n(),
            N.well.predicted = sum(Avg.Spearman > rho.threshold & 
                                     Avg.Spearman.FDR < fdr.threshold),
            .groups = "drop") %>%
  mutate(Percent.well.predicted = round(100*N.well.predicted/N.tasks, 3)) %>%
  mutate(PipeSettings.Shuffled = ifelse(PipeSettings.Shuffled, "Shuffled", "Original")) %>%
  tidyr::pivot_wider(names_from = PipeSettings.Shuffled, values_from = c(N.well.predicted, Percent.well.predicted)) 

tmp %>%
  kable() %>%
  kable_styling()

print(paste("On average,", round(mean(tmp$Percent.well.predicted_Original), 2),
            "percent of metabolites per dataset came out well predicted, compared to",
            round(mean(tmp$Percent.well.predicted_Shuffled), 2), 
            "percent when metabolite values were shuffled. Percentiles:"))

quantile(x = tmp$Percent.well.predicted_Original)
quantile(x = tmp$Percent.well.predicted_Shuffled)

rm(tmp)
```

For convenience, we split the main predictability results table into a version including only healthy datasets, only disease datasets, and all.  

```{r}
metab.pred.results.all <- metab.pred.results.sum %>%
  filter(Task.Dataset %in% datasets.to.train) %>%
  filter(Pipe.ID == chosen.pipe)

metab.pred.results.dis <- metab.pred.results.sum %>%
  filter(Task.Dataset %in% datasets.to.train) %>%
  filter(Pipe.ID == chosen.pipe) %>%
  filter(!Healthy)

metab.pred.results.sum <- metab.pred.results.sum %>%
  filter(Task.Dataset %in% datasets.to.train) %>%
  filter(Pipe.ID == chosen.pipe) %>%
  filter(Healthy)
```

### Statistics per metabolite

Aggregate statistics per metabolite:

```{r STATS_PER_METAB}
pred.per.metabolite <- metab.pred.results.sum %>%
  # Include only metabolites "common" in healthy datasets
  filter(Task.HMDB %in% common.HMDBs.healthy) %>%
  # Summarize stats per metabolite
  group_by(Task.HMDB, Compound.Name) %>%
  summarise(N.Datasets = n_distinct(Task.Dataset),
            Max.Rho = max(Avg.Spearman),
            Min.Rho = min(Avg.Spearman),
            Count.Well.Predicted = 
              sum(Avg.Spearman > rho.threshold & 
                    Avg.Spearman.FDR < fdr.threshold),
            .groups = "drop") %>%
  mutate(Perc.Well.Predicted = 
           round(100*Count.Well.Predicted/N.Datasets, 2)) %>%
  # Add all metabolite stats from HMDB
  left_join(metabolite.stats %>% select(-Compound.Name), 
            by = c("Task.HMDB" = "HMDB")) %>%
  # Order by number of datasets well-predicted in
  arrange(-Count.Well.Predicted)
```

Print summarized statistics.  

```{r STATS_PER_METAB2}
print(paste("Overall,", 
            sum(pred.per.metabolite$N.Datasets), 
            "metabolite-predictors were trained, covering",
            nrow(pred.per.metabolite), 
            "metabolites in",
            length(datasets.to.train.healthy), 
            "datasets"))
print(paste("Of these,", 
      sum(pred.per.metabolite$Count.Well.Predicted),
      "metabolite-predictors resulted in a Spearman correlation of >",rho.threshold,"and",
      "FDR value < 0.1, using loo cross validation"))
print(paste(sum(pred.per.metabolite$Count.Well.Predicted > 0),
            "metabolites were well-predicted by the above definition at least once, while",
            sum(pred.per.metabolite$Count.Well.Predicted == 0),
            "were never well predicted using our pipeline"))

print(paste("Moreover,", sum(pred.per.metabolite$Count.Well.Predicted > 1),
            "metabolites were well-predicted twice or more,",
            sum(pred.per.metabolite$Count.Well.Predicted > 2),
            "metabolites were well-predicted 3 times or more, and",
            sum(pred.per.metabolite$Count.Well.Predicted > 3),
            "metabolites were well-predicted 4 times or more."))
```

#### Compare to a shuffled version

```{r}
pred.per.metabolite.shuff <- data.frame(stringsAsFactors = F)
set.seed(21573)

for (i in 1:1000) {
  if(i%%10==0) cat('.')
  if(i%%100==0) cat('|')
  
  tmp <- metab.pred.results.sum %>%
    filter(Task.HMDB %in% common.HMDBs.healthy) %>%
    mutate(well.predicted = Avg.Spearman > rho.threshold & Avg.Spearman.FDR < fdr.threshold) %>%
    select(Task.Dataset, Task.HMDB, well.predicted) %>%
    group_by(Task.Dataset) %>% 
    mutate(well.predicted = sample(well.predicted))
  
  pred.per.metabolite.shuff <- bind_rows(pred.per.metabolite.shuff,
                                         tmp %>%
                                           # Summarize shuffled stats per metabolite
                                           group_by(Task.HMDB) %>%
                                           summarise(N.Datasets = n_distinct(Task.Dataset),
                                                     Count.Well.Predicted = sum(well.predicted),
                                                     .groups = "drop") %>%
                                           mutate(Shuffle.id = i))
}
cat('\n')

pred.per.metabolite.shuff.sum <- pred.per.metabolite.shuff %>%
  group_by(Shuffle.id) %>%
  summarise(Num.Well.Predicted.At.Least.Once = sum(Count.Well.Predicted > 0),
            Num.Well.Predicted.At.Least.2 = sum(Count.Well.Predicted > 1),
            Num.Well.Predicted.At.Least.3 = sum(Count.Well.Predicted > 2),
            Num.Well.Predicted.At.Least.4 = sum(Count.Well.Predicted > 3),
            Num.Never.Well.Predicted = sum(Count.Well.Predicted == 0))

print("Results of shuffled version:")
print("No. metabolites well-predicted at least once:")
print(paste("Average =",mean(pred.per.metabolite.shuff.sum$Num.Well.Predicted.At.Least.Once)))
quantile(x = pred.per.metabolite.shuff.sum$Num.Well.Predicted.At.Least.Once, probs = c(0, 0.025, 0.5, 0.975, 1))

print("No. metabolites well-predicted at least twice:")
print(paste("Average =",mean(pred.per.metabolite.shuff.sum$Num.Well.Predicted.At.Least.2)))
quantile(x = pred.per.metabolite.shuff.sum$Num.Well.Predicted.At.Least.2, probs = c(0, 0.025, 0.5, 0.975, 1))

print("No. metabolites well-predicted at least 3 times:")
print(paste("Average =",mean(pred.per.metabolite.shuff.sum$Num.Well.Predicted.At.Least.3)))
quantile(x = pred.per.metabolite.shuff.sum$Num.Well.Predicted.At.Least.3, probs = c(0, 0.025, 0.5, 0.975, 1))

print("No. metabolites well-predicted at least 4 times:")
print(paste("Average =",mean(pred.per.metabolite.shuff.sum$Num.Well.Predicted.At.Least.4)))
quantile(x = pred.per.metabolite.shuff.sum$Num.Well.Predicted.At.Least.4, probs = c(0, 0.025, 0.5, 0.975, 1))

print("No. metabolites never well-predicted:")
print(paste("Average =",mean(pred.per.metabolite.shuff.sum$Num.Never.Well.Predicted)))
quantile(x = pred.per.metabolite.shuff.sum$Num.Never.Well.Predicted, probs = c(0, 0.025, 0.5, 0.975, 1))

rm(tmp, i, pred.per.metabolite.shuff, pred.per.metabolite.shuff.sum)
```


### Statistics per healthy dataset

Note: sample counts are those after our processing, including the downsampling of longitudinal datasets.  

```{r STATS_PER_DATASET, fig.height=3.4, fig.width=4.8}
# --------------------- Preparations for plots --------------------->
# For plotting purposes we will sort the datasets by number of healthy samples 
tmp <- datasets.summary %>%
  filter(Dataset.subset == "Healthy") %>%
  filter(Dataset %in% 
           unique(metab.pred.results.sum$Task.Dataset)) %>%
  mutate(N.samples = as.numeric(N.samples)) %>%
  arrange(N.samples)

tmp.metab.pred.results.sum <-
  metab.pred.results.sum %>%
  inner_join(tmp %>% select(Dataset, Dataset.Original), 
             by = c("Task.Dataset" = "Dataset")) %>%
  mutate(Dataset.Original = 
           factor(Dataset.Original, 
                  levels=tmp$Dataset.Original))

healthy.datasets.order4plot <- unique(tmp$Dataset) # Keep for the next chunks

# ---------------- Plot 1: sample size per dataset ----------------->
# Create thin table with HC/full dataset sizes
tmp2 <- tmp %>%
  select(Dataset.Original, N.samples) %>%
  rename(N.samples.hc = N.samples) %>%
  left_join(datasets.summary %>% 
              filter(Dataset.subset != "All") %>% 
              group_by(Dataset.Original) %>%
              summarise(N.samples = sum(N.samples), .groups = "drop"),
             by = "Dataset.Original") %>%
  # Ordering for plot - by num of hc
  mutate(Dataset.Original = 
           factor(Dataset.Original, levels=tmp$Dataset.Original, 
                                   labels=substr(tmp$Dataset.Original,1,2))) %>%
  mutate(N.samples2 = ifelse(is.na(N.samples), 
                             N.samples.hc, 
                             N.samples)) %>%
  mutate(N.samples.label = paste0(" ",
                                  N.samples2,
                                  "\n (H=",
                                  N.samples.hc,")"))

tmp.dataset.colors <- 
  dataset.colors[names(dataset.colors) %in% 
                   datasets.to.train.healthy]
names(tmp.dataset.colors) <- 
  substr(names(tmp.dataset.colors),1,2)

p1 <- ggplot(tmp2, aes(x=Dataset.Original, y=N.samples2)) +
  geom_bar(stat="identity", color="black", fill = "grey80") +
  geom_text(aes(label=N.samples.label),
            vjust=0.5,hjust=0,size=2.8) +
  geom_bar(aes(y=N.samples.hc, fill=Dataset.Original), 
           stat="identity", color="black") +
  scale_y_continuous(expand = c(0, 0, 0.35, 0)) +
  coord_flip() +
  theme_classic() +
  scale_fill_manual(values = tmp.dataset.colors) +
  xlab("Dataset") +
  ylab("# samples\n") +
  theme(legend.position = "none",
        plot.margin = unit(c(5.5,6.5,7.8,5.5), "points"),
        axis.text.y = element_text(size = 9),
        panel.grid.major.x = 
          element_line(size = 0.5, color = "grey93"),
        axis.title.y = element_text(size = 12))

# ---- Plot 2: Number of well-predicted metabolites per dataset ---->
tmp2 <- tmp.metab.pred.results.sum %>%
  group_by(Dataset.Original) %>% 
  summarise(N.well.predicted = sum(Avg.Spearman > rho.threshold & Avg.Spearman.FDR < fdr.threshold),
            N = n(), .groups = "drop") %>%
  mutate(Perc.well.predicted = 100*N.well.predicted/N) %>%
  mutate(Perc.well.predicted2 = paste0(round(Perc.well.predicted),'%'))

print(paste("In each dataset,", min(round(tmp2$Perc.well.predicted)), '-', max(round(tmp2$Perc.well.predicted)), '% of metabolites analyzed were well-predicted'))

p2 <- ggplot(tmp2, aes(x=Dataset.Original, fill=Dataset.Original)) +
  geom_segment(aes(x = Dataset.Original, y = 0, 
                   xend = Dataset.Original, yend = N.well.predicted)) +
  geom_point(aes(y=N.well.predicted), shape=23, color="black", size=6) +
  geom_point(aes(y=N),shape=18,color="black", size=4) +
  geom_text(aes(label=Perc.well.predicted2, 
                y=ifelse(N<50, N+21, N.well.predicted+23)), 
            vjust=0.5, hjust=0, size=3) +
  scale_y_continuous(expand = c(0, 0, 0.15, 0)) +
                     # trans = log10_trans(),
                     # breaks = trans_breaks("log10", inv = function(x) 10^x, n = 5),
                     # labels = trans_format("log10", math_format(10^.x))) +
  expand_limits(y = 0) +
  coord_flip() +
  theme_classic() +
  scale_fill_manual(values = dataset.colors) +
  xlab(NULL) +
  ylab("# well-predicted\nmetabolites") +
  scale_size_manual(values = c(5, 3)) +
  theme(legend.position = "none", 
        plot.margin = unit(c(5.5,5.5,7.5,5.5), "points"),
        panel.grid.major.x = element_line(size = 0.5, color = "grey93"),
        axis.text.y = element_blank()) 

grid.arrange(p1, p2, nrow = 1, widths = c(0.56, 0.44)) 

# Clean up
rm(tmp, tmp2, p1, p2, tmp.metab.pred.results.sum, tmp.dataset.colors)
```


Distribution of number of times well predicted.

```{r fig.width=4.6, fig.height=3.4}
ggplot(pred.per.metabolite %>%
         mutate(Count.Well.Predicted = 
                  as.character(Count.Well.Predicted)) %>%
         group_by(Count.Well.Predicted, N.Datasets) %>%
         summarise(N.metabolites = n(), .groups = "drop"),
       aes(x = N.Datasets, y = N.metabolites, fill = Count.Well.Predicted)) +
  geom_bar(stat = "identity", position="stack", 
           color = "black", width=0.85) +
  scale_fill_brewer(palette = "Greys",
                    guide = guide_legend(title = "Datasets well-\npredicted in", 
                                         keywidth = 1.5, 
                                         keyheight = 0.8)) +
  scale_y_continuous(expand = c(0, 0, 0.05, 0)) +
  theme_classic() +
  xlab("Number of datasets including the metabolite") +
  ylab("Number of metabolites") + 
  theme(legend.position = c(0.86, 0.7),
        axis.title.y = element_text(size = 12))
```

### Predictability heatmaps

Plot agreement in predictability between different datasets. First prepare a temporary table for all plots.

```{r}
tmp <- metab.pred.results.sum %>%
  mutate(Task.Dataset2 = datasets.map.to.orig[Task.Dataset]) %>%
  mutate(Task.Dataset2 = factor(Task.Dataset2, 
                                levels = unname(datasets.map.to.orig[healthy.datasets.order4plot]),
                                labels = substr(unname(datasets.map.to.orig[healthy.datasets.order4plot]),1,2))) %>%
  mutate(Spearman.bin = cut(Avg.Spearman, 
                            c(-1, 0.3, 0.4, 0.5, 0.6, 1), 
                            right = F)) %>%
  mutate(Spearman.bin = 
           plyr::revalue(Spearman.bin, 
                         c("[-1,0.3)"="< 0.3", 
                           "[0.6,1)"="> 0.6"))) %>%
  mutate(Significance = get.signif.marks(Avg.Spearman.FDR)) %>%
  mutate(Significance = ifelse(Significance=="^","",Significance))
```


Metabolites that came out well-predicted 3/4 or more times:

```{r PRED_ACROSS_DATASETS, fig.width=7.6, fig.height=4.5}
HMDBs.to.include <- pred.per.metabolite %>%
  filter(Count.Well.Predicted >= 4) %>%
  pull(Task.HMDB)

p <- tmp %>%
  filter(Task.HMDB %in% HMDBs.to.include) %>%
  tidyr::complete(Task.Dataset2, Compound.Name) %>%
  ggplot(aes(y = Task.Dataset2, 
             x = Compound.Name)) +
  geom_tile(aes(fill = Spearman.bin)) +
  geom_text(aes(label = Significance), size = 3) +
  scale_fill_manual("Spearman rho", 
                      values = c("grey", brewer.pal(8, "Reds")[4:8], "white"),
                    na.value="white") +
  #scale_size_manual(name = "FDR < 0.1", values = c("TRUE" = 1, "FALSE" = 0)) +
  #scale_size(range = c(1, 20))+ 
  #guides(size = guide_legend(title=NA)) +
  xlab(NULL) +
  ylab(NULL) +
  ggtitle("Predictability of metabolites across datasets") +
  labs(subtitle = "Metabolites well predicted at least 4 times") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.3, hjust = 1, size = 9),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())

print(p)
rm(HMDBs.to.include, p)
```

Metabolites never well predicted (and in >3 datasets):

```{r PRED_ACROSS_DATASETS2, fig.width=8, fig.height=4.5}
HMDBs.to.include <- pred.per.metabolite %>%
  filter(Count.Well.Predicted == 0) %>%
  filter(N.Datasets > 3) %>%
  pull(Task.HMDB)

p <- tmp %>%
  filter(Task.HMDB %in% HMDBs.to.include) %>%
  tidyr::complete(Task.Dataset2, Compound.Name) %>%
  ggplot(aes(y = Task.Dataset2, 
             x = Compound.Name)) +
  geom_tile(aes(fill = Spearman.bin)) +
  geom_text(aes(label = Significance), size = 3) +
  scale_fill_manual("Spearman rho", 
                      values = c("grey", brewer.pal(8, "Reds")[4:8], "white"),
                    na.value="white") +
  xlab(NULL) +
  ylab(NULL) +
  ggtitle("Predictability of metabolites across datasets") +
  labs(subtitle = "Metabolites never well predicted") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.3, hjust = 1, size = 9),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())

print(p)
rm(HMDBs.to.include, p)
```

Metabolites well predicted only once (and appears in at least 3 datasets):

```{r PRED_ACROSS_DATASETS3, fig.width=11, fig.height=4.3}
HMDBs.to.include <- pred.per.metabolite %>%
  filter(Count.Well.Predicted == 1) %>%
  filter(N.Datasets > 3) %>%
  pull(Task.HMDB)

p <- tmp %>%
  filter(Task.HMDB %in% HMDBs.to.include) %>%
  tidyr::complete(Task.Dataset2, Compound.Name) %>%
  ggplot(aes(y = Task.Dataset2, 
             x = Compound.Name)) +
  geom_tile(aes(fill = Spearman.bin)) +
  geom_text(aes(label = Significance), size = 3) +
  scale_fill_manual("Spearman rho", 
                      values = c("grey", brewer.pal(8, "Reds")[4:8], "white"),
                    na.value="white") +
  xlab(NULL) +
  ylab(NULL) +
  ggtitle("Predictability of metabolites across datasets") +
  labs(subtitle = "Metabolites well predicted 1 time only") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.3, hjust = 1, size = 9),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())

print(p)
rm(HMDBs.to.include, p)
```

A combined figure with examples from each of the 3 cases above:  

```{r fig.height=3.7, fig.width=10.5}
examples.never.wp <- c("3-Hydroxycapric acid","Anserine",
                       "Caffeine","Indoleacetic acid",
                       "N-a-Acetyl-L-arginine",
                       "Propionic acid","Pyroglutamic acid",
                       "Hexadecanedioic acid",
                       "Guanine","L-Fucose")
examples.wp.once <- c("1-Methylnicotinamide",
                      "4-Hydroxybenzaldehyde",
                      "L-Proline","N2-Acetylornithine",
                      "Sphinganine",
                      "Trimethylamine N-oxide",
                      "3-Hydroxybutyric acid","Phenylacetic acid",
                      "Ursodeoxycholic acid",
                      "Myristic acid")
examples.wp.4p <- c("Azelaic acid","beta-Alanine",
                    "Dodecanedioic acid","D-Glucose",
                    "Putrescine",
                    "Sebacic acid","Taurine",
                    "Uric acid","Urobilin","L-Tyrosine")

Comps.to.include <- data.frame(Compound.Name = c(examples.never.wp, examples.wp.once, examples.wp.4p),
                               Category = factor(c(rep("Never well-predicted",10),
                                                   rep("Well-predicted once",10),
                                                   rep("Well-predicted in >3 datasets",10)),
                                                 levels = c("Never well-predicted", "Well-predicted once", "Well-predicted in >3 datasets")),
                               stringsAsFactors = FALSE)

p <- tmp %>%
  filter(Compound.Name %in% Comps.to.include$Compound.Name) %>%
  tidyr::complete(Task.Dataset2, Compound.Name) %>%
  left_join(Comps.to.include, by = "Compound.Name") %>%
  ggplot(aes(y = Task.Dataset2, 
             x = Compound.Name)) +
  geom_tile(aes(fill = Spearman.bin)) +
  geom_text(aes(label = Significance), size = 3) +
  scale_fill_manual("Spearman's \u03c1", 
                    breaks = levels(tmp$Spearman.bin),
                    values = c("grey", brewer.pal(8, "Reds")[4:8]),
                    na.value="white") +
  xlab(NULL) +
  ylab("Dataset") +
  facet_wrap(~ Category, scales = "free_x", nrow = 1) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.3, hjust = 1, size = 10),
        axis.text.y = element_text(size = 10),
        axis.title.y = element_text(size = 13),
        strip.background = element_blank(),
        strip.text.x = element_blank(),
        legend.key = element_rect(color="black"),
        panel.border = element_rect(colour = "black", fill=NA),
        panel.grid.major = element_blank(), 
        panel.margin.x = unit(0, "lines"),
        panel.grid.minor = element_blank())

print(p)
rm(Comps.to.include, p, examples.never.wp, 
   examples.wp.once, examples.wp.4p, tmp)
```

## 6. Random-effcets models (REMs)

For explanations about meta-analysis random-effects models see: *Borenstein, Michael, et al. Introduction to meta-analysis. John Wiley & Sons, 2011.* 

### Calculations

We use a random-effects model per each metabolite analyzed. The predictability estimation from each dataset, measured by Spearman's correlation, is used as the 'effect size'.  
Only healthy datasets considered here.  
We compare results from a few different random-effects settings (variance estimators, transformation from correlation r to Fisher's z, etc.).  

```{r REM_CALCULATIONS, eval=FALSE}
# ------------------ Settings and preparations --------------------->
# We will use the REM.result table to store all main results. 
# We will also store the full metacor result objects in case we need them, 
#  in "full.REM.results".
min.num.of.studies.allowed <- 3
studies.to.include <- unique(metab.pred.results.sum$Task.Dataset)

# First we get all metabolites (+ pipe settings) we want to analyze
REM.settings <- metab.pred.results.sum %>%
  select(Task.HMDB, Compound.Name) %>%
  distinct()

# We add a few other setting options
REM.settings <- merge(REM.settings, 
                     data.frame(REM.Tau.Method = c("DL", "SJ"))) 
REM.settings <- merge(REM.settings, 
                     data.frame(REM.Z.Trans = c("ZCOR"))) 
REM.settings <- merge(REM.settings, 
                     data.frame(REM.HAKN.Adjust = c(FALSE, TRUE)))  # TRUE 

full.REM.results <- list()

# ----------------- Iterate over each metabolite ------------------->
# For each compound calculate the random effects model.
REM.results <- foreach (i = 1:nrow(REM.settings), 
                        .combine=rbind,
                        .packages = c('dplyr', 'meta', 'DescTools')) %do% { 
                          
  if (i%%10==0) { cat(".") }
  if (i%%200==0) { print(paste0("Completed ", i, "/", 
                                 nrow(REM.settings), " (", 
                                 format(Sys.time(), format="%H:%M:%S"), 
                                 ")")) }

  tmp.compound.name <- REM.settings$Compound.Name[i]
  tmp.method <- REM.settings$REM.Tau.Method[i]
  tmp.z <- REM.settings$REM.Z.Trans[i] 
  tmp.hakn <- REM.settings$REM.HAKN.Adjust[i]
  
  # ----------------------- Get relevant data ---------------------->
  tmp.data <- metab.pred.results.sum %>%
    filter(Task.Dataset %in% studies.to.include) %>%
    filter(Compound.Name == tmp.compound.name) %>%
    left_join(datasets.summary %>% select(Dataset, N.subjects) %>% rename(Task.Dataset = Dataset), by = "Task.Dataset") %>%
    mutate(N.Samples.Final = ifelse(N.subjects==-1, N.Train.Samples, N.subjects))
  
  # ------- Skip if not enough studies using these settings -------->
  if (nrow(tmp.data) < min.num.of.studies.allowed) {return(NULL)}
  
  # ----------------- Calculate meta-analysis REM ------------------>
  m.cor <- metacor(cor = Avg.Spearman, 
                 n = N.Samples.Final, 
                 data = tmp.data,
                 studlab = tmp.data$Task.Dataset,
                 sm = tmp.z, # Z-transformation options (COR/ZCOR)
                 method.tau = tmp.method, # method to estimate the between-study variance 
                 hakn = tmp.hakn, 
                 comb.fixed = FALSE,
                 comb.random = TRUE, 
                 prediction = TRUE,
                 title = paste(tmp.compound.name,"predictability"))
  
  # ------------------------- Save results ------------------------->
  full.REM.results[[paste(tmp.compound.name,  
                          tmp.method, 
                          tmp.z, 
                          tmp.hakn, 
                          sep = "~")]] <- m.cor
  
  # Fill columns in summarized result table
  return(data.frame(HMDB = REM.settings$Task.HMDB[i],
                    Compound.Name = tmp.compound.name,
                    Pipe.ID = chosen.pipe,
                    Tau.Method = tmp.method,
                    Z.Transformation = tmp.z,
                    HAKN.Adjustment = tmp.hakn,
                    N.Datasets = nrow(tmp.data),
                    Predictability.rho = FisherZInv(m.cor$TE.random),
                    Lower.Predictability.rho = FisherZInv(m.cor$lower.random),
                    Upper.Predictability.rho = FisherZInv(m.cor$upper.random),
                    Pval.rho = m.cor$pval.random,
                    Lower.pred.interval = m.cor$lower.predict,
                    Upper.pred.interval = m.cor$upper.predict,
                    Q = m.cor$Q,
                    Pval.Q = m.cor$pval.Q,
                    I2 = m.cor$I2,
                    Lower.I2 = m.cor$lower.I2,
                    Upper.I2 = m.cor$upper.I2,
                    stringsAsFactors = F))
}
print("DONE!")

# ---------------------- Apply FDR correction ---------------------->
REM.results <- REM.results %>%
  group_by(Pipe.ID,
           Tau.Method,
           Z.Transformation,
           HAKN.Adjustment) %>%
  mutate(FDR.rho = p.adjust(Pval.rho, method = "fdr"))

# ----------- Save to files (in order not to recompute) ------------>
write.table(REM.results, quote = FALSE, sep = "\t", 
            file = '../data/REM_Results.tsv',
            row.names = FALSE)
save(full.REM.results, 
     file = '../data/Full_REM_Results.RData')

# ---------------------------- Clean up ---------------------------->
rm(i, tmp.compound.name, m.cor, tmp.data, REM.settings, min.num.of.studies.allowed)
```

### Organize results

```{r}
# --------- Load REM results if it didn't just run ---------->
if(!exists("REM.results")) {
  REM.results <- read.delim('../data/REM_Results.tsv', 
                            stringsAsFactors = F)
  load('../data/Full_REM_Results.RData')
}

print(paste(n_distinct(REM.results$HMDB),"metabolites were analyzed with REMs"))
```


#### Shuffled version (null hypothesis)

We use the same REM calculations, this time on shuffled versions of the data where metabolite names within each study are shuffled. We compare the number of robustly well-predicted results to the original number.  

```{r REM_CALCULATIONS_SHUFFLED, eval=FALSE}
# ------------------ Settings and preparations --------------------->
min.num.of.studies.allowed <- 3
studies.to.include <- unique(metab.pred.results.sum$Task.Dataset)

# First we get all metabolites (+ pipe settings) we want to analyze
REM.settings <- metab.pred.results.sum %>%
  select(Task.HMDB, Compound.Name) %>%
  distinct() %>%
  # We add REM setting options (dummy - we take only the settings reported in paper)
  mutate(REM.Tau.Method = "DL") %>%
  mutate(REM.Z.Trans = "ZCOR") %>%
  mutate(REM.HAKN.Adjust = FALSE)

REM.results.shuffled <- data.frame(stringsAsFactors = FALSE)

for (shuff.id in 1:1000) {
  # Track progress
  cat('.')
  if(shuff.id%%100==0) cat(shuff.id,'\n')
  
  ###############
  ## SHUFFLING ##
  ###############
  tmp.shuff.metab.pred.results <- metab.pred.results.sum %>%
    filter(Compound.Name %in% unique(pred.per.metabolite$Compound.Name)) %>% # We want the total to be 273 metabolites as above
    group_by(Task.Dataset) %>%
    mutate(Compound.Name = sample(Compound.Name))
  
  # ----------------- Iterate over each metabolite ------------------->
  # For each compound calculate the random effects model.
  tmp <- foreach (i = 1:nrow(REM.settings), 
                          .combine=rbind,
                          .packages = c('dplyr', 'meta', 'DescTools')) %do% { 
  
    tmp.compound.name <- REM.settings$Compound.Name[i]
    tmp.method <- REM.settings$REM.Tau.Method[i]
    tmp.z <- REM.settings$REM.Z.Trans[i] 
    tmp.hakn <- REM.settings$REM.HAKN.Adjust[i]
    
    # ----------------------- Get relevant data ---------------------->
    tmp.data <- tmp.shuff.metab.pred.results %>%
      filter(Task.Dataset %in% studies.to.include) %>%
      filter(Compound.Name == tmp.compound.name) %>%
      left_join(datasets.summary %>% select(Dataset, N.subjects) %>% rename(Task.Dataset = Dataset), by = "Task.Dataset") %>%
      mutate(N.Samples.Final = ifelse(N.subjects==-1, N.Train.Samples, N.subjects))
      
    # ------- Skip if not enough studies using these settings -------->
    if (nrow(tmp.data) < min.num.of.studies.allowed) {return(NULL)}
    
    # ----------------- Calculate meta-analysis REM ------------------>
    m.cor <- metacor(cor = Avg.Spearman, 
                   n = N.Samples.Final, 
                   data = tmp.data,
                   studlab = tmp.data$Task.Dataset,
                   sm = tmp.z, # Z-transformation options (COR/ZCOR)
                   method.tau = tmp.method, # method to estimate the between-study variance 
                   hakn = tmp.hakn, 
                   comb.fixed = FALSE,
                   comb.random = TRUE, 
                   prediction = TRUE,
                   title = paste(tmp.compound.name,"predictability"))
    
    # Fill columns in summarized result table
    return(data.frame(HMDB = REM.settings$Task.HMDB[i],
                      Compound.Name = tmp.compound.name,
                      N.Datasets = nrow(tmp.data),
                      Predictability.rho = FisherZInv(m.cor$TE.random),
                      Pval.rho = m.cor$pval.random,
                      I2 = m.cor$I2,
                      stringsAsFactors = F))
                          }
  
  # ---------------------- Apply FDR correction ---------------------->
  tmp <- tmp %>%
    mutate(FDR.rho = p.adjust(Pval.rho, method = "fdr")) %>%
    mutate(Shuffle.id = shuff.id)
    
  REM.results.shuffled <- bind_rows(REM.results.shuffled, tmp)
}
print("DONE!")

# ----------- Save to files (in order not to recompute) ------------>
write.table(REM.results.shuffled, quote = FALSE, sep = "\t", 
            file = '../data/REM_Results_Shuffled.tsv',
            row.names = FALSE)

# ---------------------------- Clean up ---------------------------->
rm(i, tmp.compound.name, m.cor, tmp.data, REM.settings, min.num.of.studies.allowed)
```

```{r}
# --------- Load shuffled REM results if it didn't just run ---------->
if(!exists("REM.results.shuffled")) {
  REM.results.shuffled <- read.delim('../data/REM_Results_Shuffled.tsv', stringsAsFactors = F)
}

print(paste(n_distinct(REM.results.shuffled$HMDB),"metabolites were analyzed with REMs"))

REM.results.shuffled <- REM.results.shuffled %>%
  mutate(Universal = FDR.rho < 0.1 & Predictability.rho > 0.3)

REM.results.shuffled.summary <- REM.results.shuffled %>%
  group_by(Shuffle.id) %>%
  summarise(N = n(), N.Universal = sum(Universal), .groups = "drop")

print("Results of shuffled version:")
print("No. of robustly well-predicted metabolites:")
print(paste("Average =",mean(REM.results.shuffled.summary$N.Universal)))
print(paste("SD =",sd(REM.results.shuffled.summary$N.Universal)))
quantile(x = REM.results.shuffled.summary$N.Universal, probs = c(0, 0.025, 0.5, 0.975, 1))
print("P value of true number of universal metabolites (97):")
print(sum(REM.results.shuffled.summary$N.Universal > 97)/1000)
```

### Compare REM methods

PI = Prediction Interval.  

```{r}
REM.summary.stats <- REM.results %>%
  # String with all REM settings (for presentation)
  mutate(Method = paste(Z.Transformation,
                        Tau.Method,
                        ifelse(HAKN.Adjustment,"HAKN","Uncorrected"), 
                        sep = " ~ ")) %>%
  ungroup() %>%
  select(-c(Z.Transformation, Tau.Method, HAKN.Adjustment)) %>%
  group_by(Method) %>%
  # Summary stats about meta-analysis results in different settings
  summarise(Total.Pairs.With.REMs = 
              sum(!is.na(Pval.rho), na.rm = T),
            Well.Predicted.FDR.0.05 = 
              sum(FDR.rho < 0.05 & Predictability.rho > 0.3, na.rm = T),
            Well.Predicted.FDR.0.1 = 
              sum(FDR.rho < 0.1 & Predictability.rho > 0.3, na.rm = T),
            Significant.Heterogeneity.I2.50.FDR.0.1 = 
              sum(FDR.rho < 0.1 & I2 >= 0.5, na.rm = T),
            Prediction.Interval.No.Overlap.FDR.0.1 = 
              sum(FDR.rho < 0.1 & Predictability.rho > 0.3 & 
              (Upper.pred.interval < 0 | Lower.pred.interval > 0), na.rm = T),
            .groups = "drop") 
```

```{r}
REM.summary.stats %>%
  rename(`Total num of pairs with REMs` = Total.Pairs.With.REMs) %>%
  rename(`Significant Effect FDR<0.05` = Well.Predicted.FDR.0.05) %>%
  rename(`Significant Effect FDR<0.1` = Well.Predicted.FDR.0.1) %>%
  rename(`Significant Heterogeneity I2>50% and FDR<0.1` = Significant.Heterogeneity.I2.50.FDR.0.1) %>%
  rename(`PI - No Overlap and FDR<0.1` = Prediction.Interval.No.Overlap.FDR.0.1) %>%
  kable(format = "html", escape = FALSE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"), font_size = 13) %>%
  column_spec(1:2, background = "lightgrey")
```

### Choose REM setting

We choose the DL method, and consider all the pairs with overall-effect > 0.3 and FDR < 0.1 as universal.

```{r}
selected.z <- "ZCOR"
selected.hakn <- FALSE
selected.tau <- "DL"
rho.threshold.REM <- 0.3

REM.universal <- REM.results %>% 
  filter(Pipe.ID == chosen.pipe) %>%
  filter(HAKN.Adjustment == selected.hakn) %>% 
  filter(Tau.Method == selected.tau) %>%
  filter(Z.Transformation == selected.z) %>%
  filter(FDR.rho < fdr.threshold) %>%
  filter(Predictability.rho > rho.threshold.REM) %>%
  mutate(Non.Overlapping.PI = 
           (Upper.pred.interval <= 0 | 
              Lower.pred.interval >= 0))

print(paste0("Using a cutoff of FDR < ",
             fdr.threshold,
             " and spearman rho > ", 
             rho.threshold.REM,", ", 
             nrow(REM.universal), 
             " metabolites are robustly predictable by the microbiome."))
print(paste("Of these,", 
            nrow(REM.universal %>% filter(Non.Overlapping.PI)), 
            "also have a 95% prediction interval that is completely positive. Note: prediction intervals are only relevant for comparisons of >3 studies."))
```

## 7. Explore universal microbiome-associated metabolites

### REM forest plots {.tabset}

These are the traditional REM plots.  

#### Summarized view

```{r TOP_UNIVERSAL_SUMMARIZED, fig.height=5.3, fig.width=4.6}
num.to.plot <- 20

tmp <- REM.universal %>%
  select(HMDB,Compound.Name,N.Datasets,Predictability.rho,Lower.Predictability.rho,Upper.Predictability.rho) %>%
  mutate(Rank = rank(-Predictability.rho)) %>%
  arrange(Rank) %>%
  filter(Rank <= num.to.plot) %>%
  mutate(rev.Rank = num.to.plot + 1 - Rank) %>%
  left_join(metabolites.conf.flags, by = "HMDB") %>%
  mutate(label.datasets = paste0("[",N.Datasets,ifelse(Low.Confidence.Count>0,"*",""),"]"))

tmp$Compound.Name <- factor(tmp$Compound.Name, levels = rev(tmp$Compound.Name))

tmp.polygons <- data.frame(stringsAsFactors = F)

for (i in 1:nrow(tmp)) {
  y.pos <- tmp$rev.Rank[i]
  l.p.r <- tmp$Lower.Predictability.rho[i]
  p.r <- tmp$Predictability.rho[i]
  u.p.r <- tmp$Upper.Predictability.rho[i]
  tmp.polygons <- bind_rows(tmp.polygons,
                            data.frame(HMDB = tmp$HMDB[i],
                                       Compound.Name = tmp$Compound.Name[i],
                                       x = c(l.p.r, p.r, u.p.r, p.r),
                                       y = c(y.pos, y.pos+0.25, y.pos, y.pos-0.25),
                                       stringsAsFactors = F))
}

ggplot() +
  geom_text(tmp, mapping = aes(x=Upper.Predictability.rho+0.03, y=Compound.Name, label=label.datasets), size=3.4, hjust=0, vjust=0.3) +  
  geom_vline(xintercept = 0.3, linetype="dashed", color = "red3", size=1) +
  geom_polygon(tmp.polygons, mapping = aes(x=x, y=y, group=Compound.Name), 
               color = "black", fill = "#C0CD99") +
  scale_x_continuous(limits = c(0,0.9)) +
  theme_classic() +
  ylab(NULL) +
  xlab("Predictability") +
  theme(axis.text.y = element_text(size = 10))

rm(tmp, i, tmp.polygons, p.r, l.p.r, u.p.r, y.pos, num.to.plot)
```

#### Classic forest plots

Here we print forest plots for the top 20 universal metabolites (automatic ones from metafor package).  

```{r TOP_UNIVERSAL, fig.width=9, fig.height=4.5}
# Save default setting and later restore
par.restore <- par(mar=c(4,4,1,2)) 

# ------------ Choose subset of REMs to plot ------------->
# We'll only plot the top X effects
# Note that if their are ties, it will include more than the specified n. 
REM.to.plot <- REM.universal %>% 
  top_n(n = 20, Predictability.rho) %>% 
  arrange(-Predictability.rho)

# --------- Iterate over universal correlations ---------->
for (i in 1:nrow(REM.to.plot)) {
  tmp.compound.name <- REM.to.plot$Compound.Name[i]
  tmp.rem.id <- paste(tmp.compound.name, selected.tau, selected.z, selected.hakn, sep = "~")
  tmp.m.cor <- full.REM.results[[tmp.rem.id]]
  
  # ------------------ Plot forest plot ------------------>
  forest(tmp.m.cor, 
         layout = "RevMan5", 
         col.square = "darkgoldenrod1", 
         col.square.lines = "black", 
         test.overall = TRUE)
  # Add title
  grid.text(paste(tmp.compound.name,"predictability"), 
            x=0.5, y=0.81+0.02*(REM.to.plot$N.Datasets[i]-5), 
            gp=gpar(cex=1.3))
}

# ----------------------- Clean up ----------------------->
par(par.restore)
rm(i, tmp.compound.name, tmp.m.cor, tmp.rem.id, REM.to.plot)
```

### Classes of well-predicted metabolites

We use HMDB metabolite classes to explore whether some classes are enriched in universal microbial metabolites.  We start by preparing a table useful for universal vs. non-universal comparisons (will also be used in next sections).  

```{r MAIN_VALIDATION_TABLE}
main.val.table <- REM.results %>% 
  ungroup() %>%
  filter(Pipe.ID == chosen.pipe) %>%
  filter(HAKN.Adjustment == selected.hakn) %>% 
  filter(Tau.Method == selected.tau) %>%
  filter(Z.Transformation == selected.z) %>%
  mutate(Category = ifelse(HMDB %in% unique(REM.universal$HMDB),
                           "Robustly well-predicted",
                           "Non-robustly well-predicted")) %>%
  select(HMDB, N.Datasets, Category) %>%
  left_join(metabolite.stats, by = "HMDB") %>%
  left_join(metab.pred.results.sum %>%
              group_by(Task.HMDB) %>%
              summarise(N.well.predicted = sum(Avg.Spearman > rho.threshold & 
                                                 Avg.Spearman.FDR < fdr.threshold), 
                        .groups = "drop") %>%
              rename(HMDB = Task.HMDB), by = "HMDB")

main.val.table2 <- metab.pred.results.sum %>%
  group_by(Task.HMDB) %>%
  summarise(N.datasets = n(),
            N.well.predicted = sum(Avg.Spearman > rho.threshold & Avg.Spearman.FDR < fdr.threshold),
            .groups = "drop") %>%
  mutate(Category = ifelse(N.well.predicted / N.datasets < 0.5,
                           "Well-predicted < 50%",
                           "Well-predicted >= 50%")) %>%
  rename(HMDB = Task.HMDB, N.Datasets = N.datasets) %>%
  left_join(metabolite.stats, by = "HMDB") 
```

We remove categories (e.g. classes) with less than 5 metabolites total (for plotting simplicity).  

```{r fig.width=9, fig.height=3.3}
tmp <- main.val.table %>%
  group_by(Category) %>%
  mutate(Total.category = n()) %>%
  ungroup() %>%
  group_by(Super.Class) %>%
  filter(n() > 5) %>%
  group_by(Category, Total.category, Super.Class) %>%
  summarise(N.metabolites = n(), .groups = "drop") %>%
  mutate(N.not.in.class = Total.category - N.metabolites) %>%
  rowwise() %>%
  mutate(prop = 100 * N.metabolites / Total.category,
         low = 100 * prop.test(N.metabolites, Total.category)$conf.int[1],
         upper = 100 * prop.test(N.metabolites, Total.category)$conf.int[2]) %>%
  group_by(Super.Class) %>%
  mutate(Super.Class2 = paste0(Super.Class, "\n(N = ",sum(N.metabolites), ")"))

# Add newlines for plotting
tmp$Super.Class2 <- gsub("Lipids and lipid-like molecules", "Lipids and\nlipid-like\nmolecules", tmp$Super.Class2)
tmp$Super.Class2 <- gsub("Nucleosides, nucleotides, and analogues", "Nucleosides,\nnucleotides,\nand analogues", tmp$Super.Class2)
tmp$Super.Class2 <- gsub("Organic acids and derivatives", "Organic acids\nand derivatives", tmp$Super.Class2)
tmp$Super.Class2 <- gsub("Organic nitrogen compounds", "Organic nitrogen\ncompounds", tmp$Super.Class2)
tmp$Super.Class2 <- gsub("Organic oxygen compounds", "Organic oxygen\ncompounds", tmp$Super.Class2)
tmp$Super.Class2 <- gsub("Organoheterocyclic compounds", "Organoheterocyclic\ncompounds", tmp$Super.Class2)

# Fisher's tests
cls <- unique(tmp$Super.Class)
fisher.ps <- c()
for (cl in cls) {
  tmp.f <- fisher.test(as.matrix(tmp %>% 
                          filter(Super.Class == cl) %>% 
                          ungroup() %>% 
                          select(N.metabolites, N.not.in.class)))
  print(paste("Fisher P value for",cl,":", tmp.f$p.value))
  fisher.ps <- c(fisher.ps, tmp.f$p.value)
}
fisher.fdrs <- p.adjust(fisher.ps, method = "fdr")
print("FDRs:")
print(paste(fisher.fdrs, collapse = ', '))

# Plot
# TODO: significance marks not automatic
ggplot(tmp, aes(x = Super.Class2, y = prop, fill = Category)) +
  geom_bar(stat = "identity", color = "black", position = "dodge") +
  geom_errorbar(aes(ymin=low, ymax=upper), width=.2, position=position_dodge(.9)) +
  annotate("rect", xmin = 1.75, xmax = 2.25, ymin = 51, ymax =51, alpha=1, colour = "black") +
  annotate(geom="text", x=2, y=52, label="*") +
  ggtitle("Comparison of metabolite super-classes: robustly well-predicted vs. others") +
  theme_classic() + 
  scale_fill_manual(values = c("gray87","seagreen4")) +
  xlab(NULL) +
  ylab("% of total metabolites in category") +
  theme(axis.text.x = element_text(size = 8)) +
  theme(legend.position = c(0.8, 0.75))

rm(tmp, cl, cls, fisher.ps, fisher.fdrs, tmp.f)
```

## 8. Validations {.tabset}

### Mallick et al. (MelonnPan)

Mallick et al. (2019) describe a machine learning tool for predicting metabolite levels based on microbiome profiles. As a sanity check of our pipeline, we compare the list of metabolites well-predicted by their method to well-predicted metabolites in our pipeline, using the same dataset (refered to as FRANZOSA_IBD here) as the one they use. 

Notes:
* We only use the metabolites we were able to obtain HMDB identifiers for  
* They validated their model's performance on the dutch part of the cohort, while we used LOOCV on the PRISM + dutch cohorts together.
* We used healthy subjects only while they mixed study groups
* The metabolites in white, below, were either not included due to inability to map them to HMDB identifiers, or because those metabolites are missing from our pipeline (e.g. did not appear in at least 3 datasets).

See: *Mallick, Himel, et al. "Predictive metabolomic profiling of microbial communities using amplicon or metagenomic sequences." Nature communications 10.1 (2019): 1-11.*  

```{r fig.height=4, fig.width=3}
franzosa.well.predicted.by.us <- metab.pred.results.sum %>% 
  filter(Task.Dataset == "FRANZOSA_IBD_HEALTHY") %>% 
  rename(HMDB = Task.HMDB)

melonnpan.well.predicted <- read.xls("../data/MelonnPan_supp_table_3_withHMDB.xlsx", 
                                       sheet=1, skip=1, header=TRUE)

melonnpan.well.predicted.2 <- melonnpan.well.predicted %>%
  left_join(franzosa.well.predicted.by.us, by = "HMDB") %>%
  mutate(HMDB = ifelse(HMDB=="", NA, HMDB)) %>%
  mutate(Category = ifelse(is.na(HMDB), "No mapping to HMDB",
                           ifelse(is.na(Avg.Spearman), "Not included in current analysis",
                                  ifelse(Avg.Spearman > 0.3 & Avg.Spearman.FDR < 0.1,
                                         "Well-predicted by current pipeline",
                                         "Not well-predicted by current pipeline"))))

ggplot(data.frame(table(melonnpan.well.predicted.2$Category)), 
  aes(x="", y=Freq, fill=Var1)) +
  geom_bar(width = 1, stat = "identity", color = "black") +
  coord_polar("y", start=0) +
  scale_fill_manual(values=c("No mapping to HMDB" = "white",
                             "Not included in current analysis" = "gray91",
                             "Not well-predicted by current pipeline" = "salmon3",
                             "Well-predicted by current pipeline" = 'seagreen4')) +
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.border = element_blank(),
        panel.grid=element_blank(),
        axis.ticks = element_blank(),
        axis.text.x = element_blank(),
        legend.position = "bottom",
        legend.direction = "vertical",
        legend.title=element_blank()) +
  geom_text(aes(label = percent(Freq/nrow(melonnpan.well.predicted)), x = 1.3),
            position = position_stack(vjust = 0.5), size = 4)

rm(franzosa.well.predicted.by.us, melonnpan.well.predicted, melonnpan.well.predicted.2)
```

### Zierer et al. (TwinsUK)

Here we confirm that robustly well-predicted metabolites have a larger proportion of their variance explained by the microbiome, as estimated by an independent study of the largest microbiome-metabolome dataset collected to date (twinsUK). See: *Zierer, Jonas, et al. "The fecal metabolome as a functional readout of the gut microbiome." Nature genetics 50.6 (2018): 790-795.*  

For this purpose we use a supplementary table from the paper, and mappings to HMDB ID's.

```{r TWINSUK_VALIDATION_VAR_EXPLAINED}
twinsUK.metab.mappings <- read_delim("../data/Zierer_et_al_2018/Metabolite_HMDB_Mappings.tsv", 
                                       "\t", escape_double = FALSE, trim_ws = TRUE)
twinsUK.MM.var.explained <- read.xls("../data/Zierer_et_al_2018/Variance components and phenotype associations of fecal metabolites.xlsx", 
                                       sheet=1, skip=1, header=TRUE)

twinsUK.MM.var.explained <- twinsUK.MM.var.explained %>%
  select(ID, Metabolite, M, p) %>%
  mutate(var.explained = as.numeric(gsub(" \\(.*\\)", "", M))) %>%
  # We use a pre-calculated mapping to HMDB ID's using MetaboAnalyst
  left_join(twinsUK.metab.mappings %>% 
              select(Metabolite, HMDB) %>% 
              distinct() %>% 
              rename(ID = Metabolite), by = "ID")
```

```{r fig.width=3, fig.height=4}
tmp <- REM.results %>%
  ungroup() %>%
  filter(Pipe.ID == chosen.pipe) %>%
  filter(HAKN.Adjustment == selected.hakn) %>% 
  filter(Tau.Method == selected.tau) %>%
  filter(Z.Transformation == selected.z) %>%
  select(HMDB, Predictability.rho) %>%
  inner_join(twinsUK.MM.var.explained %>% select(HMDB, var.explained), by = "HMDB") %>%
  mutate(Universal = ifelse(HMDB %in% REM.universal$HMDB, "Robustly\nwell-predicted", "Non-robustly\nwell-predicted"))

# Continuous to binary
(tmp2 <- wilcox.test(tmp$var.explained~tmp$Universal))

ggplot(tmp, aes(x=Universal, fill = Universal, 
                y=var.explained)) +
  geom_boxplot(outlier.shape = NA)+
  geom_jitter(shape=16, alpha = 0.5, 
              position=position_jitter(0.2)) +
  scale_fill_manual(values = c("gray77","seagreen4")) +
  ylab("Metabolite variance explained by the microbiome\n(Zierer et al. 2019)") +
  xlab(NULL) + #xlab("Robustly well-predicted\nby the microbiome") +
  annotate("rect", ymin = 1.04, ymax = 1.04, xmin = 1, xmax = 2, alpha=1, colour = "black") +
  annotate(geom="text", y=1.06, x=1.5, label=get.signif.marks(tmp2$p.value)) +
  theme_classic() +
  theme(legend.position = "none")

rm(tmp, tmp2)
```

## 9. Subgroups REMs

### Calculations - Healthy vs. Disease

We only consider metabolites that appeared at least 3 times in the healthy datasets and 3 times in disease.   

```{r REM_SUBGROUP_CALCULATIONS, eval=FALSE}
# ------------------ Settings and preparations --------------------->
# Get metabolites to analyze - at least 3 times in health...
REM.settings <- metab.pred.results.sum %>%
  group_by(Task.HMDB, Compound.Name) %>%
  filter(n() >= 3) %>%
  select(Task.HMDB, Compound.Name) %>%
  distinct()
n1 <- nrow(REM.settings)

# ... and 3 times in disease
keep.these <- metab.pred.results.dis %>%
  group_by(Task.HMDB, Compound.Name) %>%
  filter(n() >= 3) %>%
  select(Task.HMDB, Compound.Name) %>%
  distinct()

REM.settings <- REM.settings %>%
  inner_join(keep.these, by = c("Task.HMDB", "Compound.Name"))

print(paste("Out of", n1, "metabolites analyzed in the healthy datasets,",
            nrow(REM.settings)," also appeared 3 or more times in disease datasets.",
            "These will participate in the subgroup analysis."))

# We add a few other setting options
REM.settings <- merge(REM.settings, 
                     data.frame(REM.Tau.Method = c("DL"))) #"PM", "SJ"
REM.settings <- merge(REM.settings, 
                     data.frame(REM.Z.Trans = c("ZCOR"))) # "COR"
REM.settings <- merge(REM.settings, 
                     data.frame(REM.HAKN.Adjust = c(FALSE)))  # TRUE 

# Place holder for combined REM (REM that simply includes both healthy and disease datasets) and subgroup REM (which further analyzes differences between groups)
healthy.disease.REM.results <- list()
subgroup.REM.results <- list()

# ----------------- Iterate over each metabolite ------------------->
# For each compound calculate the random effects model.
REM.subgroup.results <- foreach (i = 1:nrow(REM.settings), 
                        .combine=rbind,
                        .packages = c('dplyr', 'meta', 'DescTools')) %do% { # i = 161
                          
  if (i%%10==0) { cat(".") }
  if (i%%100==0) { print(paste0("Completed ", i, "/", 
                                 nrow(REM.settings), " (", 
                                 format(Sys.time(), format="%H:%M:%S"), 
                                 ")")) }

  tmp.compound.name <- REM.settings$Compound.Name[i]
  tmp.method <- REM.settings$REM.Tau.Method[i]
  tmp.z <- REM.settings$REM.Z.Trans[i] 
  tmp.hakn <- REM.settings$REM.HAKN.Adjust[i]
  
  # ----------------------- Get relevant data ---------------------->
  tmp.data <- bind_rows(metab.pred.results.sum,
                        metab.pred.results.dis) %>%
    filter(Compound.Name == tmp.compound.name) %>%
    left_join(datasets.summary %>% 
                select(Dataset, N.subjects) %>% 
                rename(Task.Dataset = Dataset), 
              by = "Task.Dataset") %>%
    mutate(N.Samples.Final = ifelse(N.subjects==-1, N.Train.Samples, N.subjects))
  
  # ----------------- Calculate meta-analysis REM ------------------>
  m.cor <- metacor(cor = Avg.Spearman, 
                 n = N.Samples.Final, 
                 data = tmp.data,
                 studlab = tmp.data$Task.Dataset,
                 sm = tmp.z, # Z-transformation options (COR/ZCOR)
                 method.tau = tmp.method, # method to estimate the between-study variance 
                 hakn = tmp.hakn, 
                 comb.fixed = FALSE,
                 comb.random = TRUE, 
                 prediction = TRUE,
                 title = paste(tmp.compound.name,"predictability"))
  
  # ------------------------- Save results ------------------------->
  analysis.id <- paste(tmp.compound.name,  
                          tmp.method, 
                          tmp.z, 
                          tmp.hakn, 
                          sep = "~")
  healthy.disease.REM.results[[analysis.id]] <- m.cor
  
   # ------------------------ Get subgroups ------------------------>
  # Replaces the name of each study included in this meta-analysis 
  #  object with a label "HEALTHY" or "DISEASE".
  subgroups <- sapply(strsplit(m.cor$studlab, split = "_"), last)
   
  # ---------------- Run subgroup meta analysis -------------------->
  meta.subgroups <- subgroup.analysis.mixed.effects2(x = m.cor, 
                                                     subgroups = subgroups)
  subgroup.REM.results[[analysis.id]] <- meta.subgroups
  
  # Fill columns in summarized result table
  return(data.frame(HMDB = REM.settings$Task.HMDB[i],
                    Compound.Name = tmp.compound.name,
                    Pipe.ID = chosen.pipe,
                    Tau.Method = tmp.method,
                    Z.Transformation = tmp.z,
                    HAKN.Adjustment = tmp.hakn,
                    N.Datasets = nrow(tmp.data),
                    N.Healthy.Datasets = sum(subgroups == "HEALTHY"),
                    N.Disease.Datasets = sum(subgroups != "HEALTHY"),
                    Overall.Predictability.rho = FisherZInv(m.cor$TE.random),
                    Overall.Lower.Predictability.rho = FisherZInv(m.cor$lower.random),
                    Overall.Upper.Predictability.rho = FisherZInv(m.cor$upper.random),
                    Overall.Pval.rho = m.cor$pval.random,
                    Overall.Lower.pred.interval = m.cor$lower.predict,
                    Overall.Upper.pred.interval = m.cor$upper.predict,
                    Overall.Q = m.cor$Q,
                    Overall.Pval.Q = m.cor$pval.Q,
                    Overall.I2 = m.cor$I2,
                    Overall.Lower.I2 = m.cor$lower.I2,
                    Overall.Upper.I2 = m.cor$upper.I2,
                    Healthy.rho = meta.subgroups$within.subgroup.results["HEALTHY","TE"],
                    Healthy.rho.lower.ci = meta.subgroups$within.subgroup.results["HEALTHY","LLCI"],
                    Healthy.rho.upper.ci = meta.subgroups$within.subgroup.results["HEALTHY","ULCI"],
                    Healthy.Pval = meta.subgroups$within.subgroup.results["HEALTHY","p"],
                    Healthy.I2 = meta.subgroups$within.subgroup.results["HEALTHY","I2"],
                    Disease.rho = meta.subgroups$within.subgroup.results["DISEASE","TE"],
                    Disease.rho.lower.ci = meta.subgroups$within.subgroup.results["DISEASE","LLCI"],
                    Disease.rho.upper.ci = meta.subgroups$within.subgroup.results["DISEASE","ULCI"],
                    Disease.Pval = meta.subgroups$within.subgroup.results["DISEASE","p"],
                    Disease.I2 = meta.subgroups$within.subgroup.results["DISEASE","I2"],
                    Between.Subgroups.Q = meta.subgroups$subgroup.analysis.results$Q,
                    Between.Subgroups.Pval = meta.subgroups$subgroup.analysis.results$p,
                    stringsAsFactors = F))
}
print("DONE!")

# ---------------------- Apply FDR correction ---------------------->
REM.subgroup.results$Overall.FDR.rho <- p.adjust(REM.subgroup.results$Overall.Pval.rho, method = "fdr")
REM.subgroup.results$Healthy.FDR <- p.adjust(REM.subgroup.results$Healthy.Pval, method = "fdr")
REM.subgroup.results$Disease.FDR <- p.adjust(REM.subgroup.results$Disease.Pval, method = "fdr")
REM.subgroup.results$Between.Subgroups.FDR <- p.adjust(REM.subgroup.results$Between.Subgroups.Pval, method = "fdr")

# ----------- Save to files (in order not to recompute) ------------>
write.table(REM.subgroup.results, quote = FALSE, sep = "\t", 
            file = '../data/Subgroup_REM_Results.tsv',
            row.names = FALSE)
save(list = c("subgroup.REM.results","healthy.disease.REM.results"), 
     file = '../data/Full_Subgroup_REM_Results.RData')

# ---------------------------- Clean up ---------------------------->
rm(i, tmp.compound.name, m.cor, tmp.data, REM.settings, keep.these, tmp.method,  tmp.hakn, tmp.z)
```

### Organize results

```{r REM_SUBGROUP_ORGANIZE}
# --------- Load REM results if it didn't just run ---------->
if(!exists("REM.subgroup.results")) {
  REM.subgroup.results <- read.delim('../data/Subgroup_REM_Results.tsv', stringsAsFactors = F)
  load('../data/Full_Subgroup_REM_Results.RData')
}

print(paste(n_distinct(REM.subgroup.results$HMDB),"metabolites were analyzed in the subgroup REM analysis"))
```

### Custom plots

```{r CUSTOM_SUBGROUP_REM_PLOTS, fig.height=7, fig.width=8}
tmp <- REM.subgroup.results %>%
  filter(HMDB %in% REM.universal$HMDB[REM.universal$Predictability.rho>0.445])

tmp1 <- tmp %>%
  select(HMDB,Compound.Name,N.Healthy.Datasets,Healthy.rho,Healthy.rho.lower.ci,Healthy.rho.upper.ci) %>%
  rename(N.Datasets = N.Healthy.Datasets) %>%
  rename(Predictability.rho = Healthy.rho) %>%
  rename(Lower.Predictability.rho = Healthy.rho.lower.ci) %>%
  rename(Upper.Predictability.rho = Healthy.rho.upper.ci) %>%
  mutate(h.d. = "Healthy") %>%
  mutate(y.pos = 2) 

tmp2 <- tmp %>%
  select(HMDB,Compound.Name,N.Disease.Datasets,Disease.rho,Disease.rho.lower.ci,Disease.rho.upper.ci) %>%
  rename(N.Datasets = N.Disease.Datasets) %>%
  rename(Predictability.rho = Disease.rho) %>%
  rename(Lower.Predictability.rho = Disease.rho.lower.ci) %>%
  rename(Upper.Predictability.rho = Disease.rho.upper.ci) %>%
  mutate(h.d. = "Disease") %>%
  mutate(y.pos = 3) 

tmp3 <- tmp %>%
  select(HMDB,Compound.Name,Overall.Predictability.rho,
         Overall.Lower.Predictability.rho, Overall.Upper.Predictability.rho) %>%
  rename(Predictability.rho = Overall.Predictability.rho) %>%
  rename(Lower.Predictability.rho = Overall.Lower.Predictability.rho) %>%
  rename(Upper.Predictability.rho = Overall.Upper.Predictability.rho) %>%
  mutate(h.d. = "Overall") %>%
  mutate(y.pos = 1) %>%
  arrange(-Predictability.rho)

tmp <- bind_rows(tmp1, tmp2, tmp3) %>%
  left_join(metabolites.conf.flags, by = "HMDB") %>%
  mutate(label.datasets = ifelse(h.d. == "Overall",
                                 "",
                                 paste0("[",N.Datasets,ifelse(Low.Confidence.Count>0,
                                                              "*",""),
                                        "]"))) %>%
  mutate(Compound.Name = factor(Compound.Name, levels = tmp3$Compound.Name)) %>%
  mutate(h.d. = factor(h.d., levels = c("Overall", "Healthy", "Disease")))

# Break some of the long metabolite names
levels(tmp$Compound.Name)[levels(tmp$Compound.Name)=="Dihomo-gamma-linolenic acid"] <- "Dihomo-gamma-\nlinolenic acid"
levels(tmp$Compound.Name)[levels(tmp$Compound.Name)=="Docosapentaenoic acid (22n-6)"] <- "Docosapentaenoic\nacid (22n-6)"
levels(tmp$Compound.Name)[levels(tmp$Compound.Name)=="7-Ketodeoxycholic acid"] <- "7-Ketodeoxycholic\nacid"
levels(tmp$Compound.Name)[levels(tmp$Compound.Name)=="Symmetric dimethylarginine"] <- "Symmetric\ndimethylarginine"
levels(tmp$Compound.Name)[levels(tmp$Compound.Name)=="N-Acetyl-L-glutamic acid"] <- "N-Acetyl-L-\nglutamic acid"
levels(tmp$Compound.Name)[levels(tmp$Compound.Name)=="Asymmetric dimethylarginine"] <- "Asymmetric\ndimethylarginine"
levels(tmp$Compound.Name)[levels(tmp$Compound.Name)=="N1,N12-Diacetylspermine"] <- "N1,N12-\nDiacetylspermine"
levels(tmp$Compound.Name)[levels(tmp$Compound.Name)=="Linoleoyl ethanolamide"] <- "Linoleoyl\nethanolamide"

tmp.polygons <- data.frame(stringsAsFactors = F)

for (i in 1:nrow(tmp)) {
  y.pos <- tmp$y.pos[i]
  l.p.r <- tmp$Lower.Predictability.rho[i]
  p.r <- tmp$Predictability.rho[i]
  u.p.r <- tmp$Upper.Predictability.rho[i]
  tmp.polygons <- bind_rows(tmp.polygons,
                            data.frame(HMDB = tmp$HMDB[i],
                                       Compound.Name = tmp$Compound.Name[i],
                                       h.d = tmp$h.d.[i],
                                       x = c(l.p.r, p.r, u.p.r, p.r),
                                       y = c(y.pos, y.pos+0.25, y.pos, y.pos-0.25),
                                       stringsAsFactors = F))
}

ggplot() +
  geom_text(tmp, mapping = aes(x=Upper.Predictability.rho+0.03, 
                               y=y.pos, label=label.datasets), 
            size=3.4, hjust=0, vjust=0.3) +
  geom_vline(xintercept = 0.3, linetype="dashed", color = "red3", size=1) +
  geom_polygon(tmp.polygons,
               mapping = aes(x=x, y=y, group=h.d, fill = h.d),
               colour="black") +
  scale_x_continuous(limits = c(0,0.91)) +
  scale_y_continuous(breaks = 1:3, labels = levels(tmp$h.d.)) +
  scale_fill_manual(values = c("Healthy" = "#C0CD99", "Disease" = "#e26d5c", "Overall" = "black")) +
  theme_bw() +
  ylab(NULL) +
  xlab("Predictability") +
  facet_wrap(~ Compound.Name, ncol = 2, strip.position="right") +
  theme(axis.text.y = element_text(size = 9.5),
        strip.text.y = element_text(angle=0, hjust = 0),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        strip.background = element_blank(), #element_rect(colour = "black", fill = "lightgrey"),
        legend.position = "none")

rm(tmp, tmp1, tmp2, tmp3, i, tmp.polygons, p.r, l.p.r, u.p.r, y.pos)
```

## 10. Models comparison analysis - healthy only

For each metabolite well-predicted 3 or more times we take all healthy datasets that had well-performing models for them.   
We perform 2 types of analysis:  

(1) Comparison of important features: we first calculate permutation-based feature importance scores for each model included.
We use the method from the reference below to compute p values for our feature importances (implemented in the ranger package).  
*Altmann, Andr, et al. "Permutation importance: a corrected feature importance measure." Bioinformatics 26.10 (2010): 1340-1347.*   
Different datasets have different sets of features and so we also compare feature importance scores of models after re-training the models so that they only includes features shared by both models compared (we call this the pairwise-adjusted version).   

(2) Pairwise cross-predictability.  

### Feature importance

We re-train one final model (using all samples available) for each metabolite *well-predicted* in each dataset, and analyze its feature contributions using permutation based scores and p values (see reference above).   

As before - we run the pipeline several times for each task in order to check our results stability.   

```{r}
# Options
genera.trans.choice = "RELATIVE"
model.choice = "RF"
rho.threshold.on.loocv <- 0.3
p.threshold.on.loocv <- 0.1
```


```{r CONTRIBUTORS_PERMUTATION, eval = FALSE, message=FALSE}
# Place holder for feature importance
metab.contributors.Perm <- data.frame(stringsAsFactors = F)
final.rf.models <- list()

# Iterate over datasets
for (dataset in datasets.to.train) { 
  cat("Working on dataset:", dataset, "\n")
  print(format(Sys.time(), format="%H:%M:%S"))
  dataset.type <- ifelse(grepl("HEALTHY", dataset), "HEALTHY", "DISEASE")
  dataset.orig <- gsub("(_HEALTHY)|(_DISEASE)","",dataset)
  
  # Get list of metabolites to work on (metabolites with HMDB ID's + well predicted)
  # Note that we actually collect here feature importances for any well-performing model, but we later limit the analysis to those metabolites well-predicted in 3 datasets at least
  if (dataset.type == "HEALTHY") { 
    get.metabs.from.table <- metab.pred.results.sum
  } else { get.metabs.from.table <- metab.pred.results.dis }
  metabs.for.training <- get.metabs.from.table %>%
    filter(Task.Dataset == dataset) %>%
    filter(Avg.Spearman > rho.threshold.on.loocv) %>%
    filter(Avg.Spearman.FDR < p.threshold.on.loocv) %>%
    select(Task.HMDB, Task.Metabolite.Raw) %>%
    filter(Task.HMDB %in% REM.universal$HMDB)
  print(paste(nrow(metabs.for.training), "metabolites in this dataset will be analyzed."))
  if (nrow(metabs.for.training) == 0) {next}
  
  # Prepare feature table (genera abundances)
  genera <- get.feature.table(dataset, 
                              genera.trans, 
                              genera.trans.choice)
      
  # Iterate over metabolites
  for (i in 1:nrow(metabs.for.training)) { 
    cat(".")
    
    # Get metabolite name
    metabolite <- metabs.for.training$Task.Metabolite.Raw[i]
    metabolite.hmdb <- metabs.for.training$Task.HMDB[i]
    
    # Add target variable (metabolite values) to feature table
    ## Patch: for some reason some trailing white-spaces have been removed in some versions of metabolite name, so we optionally add it back if needed
    if (! metabolite %in% rownames(mtb.trans$LOG[[dataset]])) { 
      metabolite <- paste0(metabolite, ' ') 
    }
    metab.vector <- 
      mtb.trans$LOG[[dataset]][metabolite, rownames(genera$dt)]
    # Scale (zero-mean unit-variance)
    metab.vector.scaled <- c(scale(metab.vector))
    dt <- cbind(genera$dt, Metabolite = metab.vector.scaled)
    
    # To check how "stable" our pipeline is, we run it <num.runs> times per task
    for (run in 1:num.runs) { 
      
      # Specify model 
      model <- my.get.model(model.choice, 
                            rf.importance = "permutation") 
      
      # Create workflow object (tidymodels convention)
      workflow.obj <- my.get.workflow(model)
      
      # Specify a "dummy" hyper-parameters grid (default params)
      hyparam.grid <- my.get.hyperparameter.grid(model.choice, n.features = genera$n, dummy = TRUE)
      
      # Train final model using entire train dataset
      final.model <- 
        workflow.obj %>% 
        finalize_workflow(parameters = hyparam.grid) %>% 
        fit(data = dt)
      # Save it
      final.rf.models[[paste(dataset.orig, dataset.type, metabolite.hmdb, run, sep="~")]]
      
      # Get permutation-based feature importance
      feat.importance.Perm <- 
        data.frame(importance_pvalues(final.model$fit$fit$fit, 
                                      method = "altmann", 
                                      formula = Metabolite ~ ., 
                                      data = dt))
      names(feat.importance.Perm) <- c("Score", "P.Value")
      feat.importance.Perm$Feature <- rownames(feat.importance.Perm)
      rownames(feat.importance.Perm) <- NULL
      feat.importance.Perm <- feat.importance.Perm %>% arrange(-Score)
      feat.importance.Perm$Task.Dataset <- dataset
      feat.importance.Perm$Task.Metabolite.Raw <- metabolite
      feat.importance.Perm$Task.HMDB <- metabolite.hmdb
      feat.importance.Perm$Run <- run
      
      # Update main results table
      metab.contributors.Perm <- bind_rows(metab.contributors.Perm, feat.importance.Perm)
    } # Done iterating over runs
  } # Done iterating over metabs
} # Done iterating over datasets

# Save results to file
write.table(metab.contributors.Perm,
          file = '../data/Permutation_Feature_Importances.tsv',
          row.names = FALSE, sep = "\t", quote = FALSE)

# Clean up
rm(dataset, i, metabolite, genera.trans.choice, dt, metab.vector, model.choice, genera, workflow.obj, model, metabs.for.training, run, hyparam.grid, final.model, tmp.warnings, get.metabs.from.table, dataset.type, dataset.orig, feat.importance.Perm)
```

#### Organize raw results

Load results.  

```{r message = FALSE}
metab.contributors.Perm <- read_delim("../data/Permutation_Feature_Importances.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

# If results include datasets no longer relevant, remove them now
metab.contributors.Perm <- metab.contributors.Perm %>%
  filter(Task.Dataset %in% datasets.to.train) %>%
  filter(Task.HMDB %in% common.HMDBs)

print("Loaded contribution analysis results from file")
```

Aggregate results over runs.  

```{r}
# Average importance scores / p values over runs for final ranks
contribs <- metab.contributors.Perm %>%
  # Only universal
  filter(Task.HMDB %in% REM.universal$HMDB) %>%
  # Average scores over runs
  group_by(Task.Dataset, Task.Metabolite.Raw, Task.HMDB, Feature) %>%
  summarise(N.sanity.5 = n(), Mean.Score = mean(Score), Mean.P = mean(P.Value), Var.P = var(P.Value), .groups = "drop") %>%
  # Add ranks
  group_by(Task.Dataset, Task.Metabolite.Raw, Task.HMDB) %>%
  mutate(Feat.Rank = rank(Mean.P)) %>%
  ungroup() %>%
  left_join(metabolite.stats[,c("HMDB","Compound.Name")], 
            by = c("Task.HMDB" = "HMDB")) %>%
  mutate(Healthy = grepl("HEALTHY", Task.Dataset))

# Here we create a list of metabolites to further look into in the context of comparing models (i.e. only metabolites well-predicted in at least 3 individual studies)
metabs.for.contrib.analysis <- unique(contribs %>%
                           filter(Healthy) %>%
                           group_by(Task.HMDB) %>%
                           filter(n_distinct(Task.Dataset) >= 3) %>%
                           ungroup() %>%
                           pull(Task.HMDB))

print(paste("We analyze contribution comparison for", 
            length(metabs.for.contrib.analysis),
            "metabolites that were well-predicted 3 or more times and universal"))

rm(metab.contributors.Perm)
```

### Cross-predictability & pairwise-adjusted contributors

For the cross-predictability analysis we down-sample the larger dataset to the size of the smaller dataset.  

TODO: current code runs some redundant model training tasks. Need to re-organize and make efficient.  

```{r PAIRWISE_HC_CONTRIBS_CROSS_PREDS, warning=FALSE, eval=FALSE}
metab.contributors2 <- data.frame(stringsAsFactors = F)
cross.pred.results2 <- data.frame(stringsAsFactors = F)

for (tmp.hmdb in metabs.for.contrib.analysis) { 
  tmp.metab.name <- metabolite.stats$Compound.Name[metabolite.stats$HMDB == tmp.hmdb]
  cat("\n"); print(paste("Analyzing:", tmp.metab.name))
  
  # Get names of datasets where this metabolite was well-predicted to begin with
  relevant.datasets <- unique(contribs %>% filter((Task.HMDB == tmp.hmdb) & Healthy) %>% pull(Task.Dataset))
  
  for (flag.re.normalize in c(TRUE)) { 
    print(paste(ifelse(flag.re.normalize,"With","Without"), "re-normalizing"))
    
    # TODO: re-organize. 
    for (flag.down.sample in c(FALSE, TRUE)) { 
      
      # Iterate over pairs of datasets
      for (i in 1:length(relevant.datasets)) { 
        for (j in 1:length(relevant.datasets)) { 
          if (i == j) next;
          cat(i, "&", j)
          dataset1 <- relevant.datasets[i]
          dataset2 <- relevant.datasets[j]
          
          raw.metab.name1 <- metab.pred.results.sum %>% filter(Task.Dataset == dataset1 & Task.HMDB == tmp.hmdb) %>% pull(Task.Metabolite.Raw)
          raw.metab.name2 <- metab.pred.results.sum %>% filter(Task.Dataset == dataset2 & Task.HMDB == tmp.hmdb) %>% pull(Task.Metabolite.Raw)
          
          # Prepare feature table (genera abundances)
          genera1 <- get.feature.table(dataset1, genera.trans, genera.trans.choice)
          genera2 <- get.feature.table(dataset2, genera.trans, genera.trans.choice)
          
          # Get only shared features
          shared.genera.feats <- intersect(colnames(genera1$dt), colnames(genera2$dt))
          dt1 <- genera1$dt[,shared.genera.feats]
          dt2 <- genera2$dt[,shared.genera.feats]
          
          # Re-normalize to relative abundances if needed
          if (flag.re.normalize) {
            dt1 <- decostand(dt1, method = "total", MARGIN = 1)
            dt2 <- decostand(dt2, method = "total", MARGIN = 1)
          }
          
          # Sample sizes
          n1 <- nrow(dt1)
          n2 <- nrow(dt2)
            
          # Add target variable (metabolite values) to dataset 1 feature table
          # Patch: for some reason some trailing white-spaces have been removed in some versions of metabolite name, so we optionally add it back if needed
          if (! raw.metab.name1 %in% rownames(mtb.trans$LOG[[dataset1]])) {raw.metab.name1 <- paste0(raw.metab.name1, ' ')}
          metab.vector <- mtb.trans$LOG[[dataset1]][raw.metab.name1, rownames(dt1)]
          metab.vector.scaled <- c(scale(metab.vector)) # Scale
          dt1 <- cbind(dt1, Metabolite = metab.vector.scaled)
          
          # Add target variable - dataset 2
          if (! raw.metab.name2 %in% rownames(mtb.trans$LOG[[dataset2]])) {raw.metab.name2 <- paste0(raw.metab.name2, ' ')}
          metab.vector <- mtb.trans$LOG[[dataset2]][raw.metab.name2, rownames(dt2)]
          metab.vector.scaled <- c(scale(metab.vector)) # Scale
          dt2 <- cbind(dt2, Metabolite = metab.vector.scaled)
          
          # For longitudinal datasets, get a named vector that will serve as a map between sample names to subject names
          # If the dataset is not longitudinal, the map is from sample name to sample name. (so it can be effectively treated the same)
          # --> dataset1
          sample.to.subject1 <- get.sample.to.subject(dataset1, metadatas, mtb.trans)
          
          # --> dataset2
          sample.to.subject2 <- get.sample.to.subject(dataset2, metadatas, mtb.trans)
          
          # To check how "stable" our pipeline is, we run it X times per task
          for (run in 1:10) { 
            cat('.')
            # If needed, subsample larger study group to match size of smaller study group (ds = down-sampled)
            if (flag.down.sample) {
              n.min <- min(n1, n2)
              dt1.ds <- dt1[sample(x = n1, size = n.min),]
              dt2.ds <- dt2[sample(x = n2, size = n.min),]
            } else {
              dt1.ds <- dt1
              dt2.ds <- dt2
            }
            
            # Update mappings sample to subject
            tmp.sample.to.subject1 <- sample.to.subject1[names(sample.to.subject1) %in% rownames(dt1.ds)]
            tmp.sample.to.subject2 <- sample.to.subject2[names(sample.to.subject2) %in% rownames(dt2.ds)]
            tmp.unique.subjects1 <- unique(unname(tmp.sample.to.subject1))
            
            # Train predictor for dataset1
            model <- my.get.model(model.choice, rf.importance = "permutation")
            workflow.obj <- my.get.workflow(model)
            hyparam.grid <- my.get.hyperparameter.grid(model.choice, n.features = length(shared.genera.feats), dummy = TRUE)
            
            # Collect CV performance metrics 
            loocv.preds <- foreach(ii = 1:length(tmp.unique.subjects1), 
                                         .combine = rbind, 
                                         .packages = c('tidymodels')) %dopar% { 
              tmp.samples.to.test.on <- names(tmp.sample.to.subject1)[tmp.sample.to.subject1 == tmp.unique.subjects1[ii]]
              tmp.trained.model <- workflow.obj %>% 
                finalize_workflow(parameters = hyparam.grid) %>% 
                fit(data = dt1.ds[! rownames(dt1.ds) %in% tmp.samples.to.test.on,])
              my.predict(tmp.trained.model, dt1.ds[sample(tmp.samples.to.test.on, 1),])
            }
            loocv.results <- my.get.metrics(loocv.preds)
            names(loocv.results) <- paste0("loocv.", names(loocv.results))
        
            # Now train final model using entire train dataset
            final.model <- 
              workflow.obj %>% 
              finalize_workflow(parameters = hyparam.grid) %>% 
              fit(data = dt1.ds)
            
            if (flag.down.sample) {
              # Get predictions on test data (dataset2)
              test.preds <- my.predict(final.model, dt2.ds)
          
              # Calculate test performance (for longitudinal datasets, we pick here one sample per subject)
              tmp.sample.to.subject2 <- sample(tmp.sample.to.subject2)
              tmp.sample.to.subject2 <- tmp.sample.to.subject2[! duplicated(tmp.sample.to.subject2)]
              test.preds <- test.preds[names(tmp.sample.to.subject2),]
              test.performance <- my.get.metrics(test.preds)
              
              # Extract previous performance metrics on train set using cv (for later sanity)
              prev.loocv.results1 <- metab.pred.results.sum %>%
                filter(Task.Dataset == dataset1 & Task.HMDB == tmp.hmdb)  
              prev.loocv.results2 <- metab.pred.results.sum %>%
                filter(Task.Dataset == dataset2 & Task.HMDB == tmp.hmdb)
              
              cross.pred.results2 <- bind_rows(cross.pred.results2,
                                               data.frame(Compound.Name = tmp.metab.name,
                                                          HMDB = tmp.hmdb,
                                                          Dataset1 = dataset1,
                                                          Dataset2 = dataset2,
                                                          N.features1 = genera1$n,
                                                          N.features2 = genera2$n,
                                                          N.shared.features = length(shared.genera.feats),
                                                          # Numbers before down-sampling
                                                          N.samples1 = n1,
                                                          N.unique.subjects1 = n_distinct(sample.to.subject1),
                                                          N.samples2 = n2,
                                                          N.unique.subjects2 = n_distinct(sample.to.subject2),
                                                          Re.normalized = flag.re.normalize,
                                                          Down.sampled = flag.down.sample,
                                                          Run = run,
                                                          Prev.Spearman.rho.cv1 = 
                                                             prev.loocv.results1$Avg.Spearman,
                                                          Prev.Spearman.fdr.cv1 = 
                                                             prev.loocv.results1$Avg.Spearman.FDR,
                                                          Prev.Spearman.rho.cv2 = 
                                                             prev.loocv.results2$Avg.Spearman,
                                                          Prev.Spearman.fdr.cv2 = 
                                                             prev.loocv.results2$Avg.Spearman.FDR,
                                                          # Updated results using LOOCV on dataset1
                                                          New.Spearman.rho.cv1 = 
                                                             loocv.results$loocv.spearman.rho,
                                                          New.Spearman.p.cv1 = 
                                                             loocv.results$loocv.spearman.p,
                                                          # Cross-prediction results on dataset2
                                                          New.Spearman.rho.cp2 = 
                                                             test.performance$spearman.rho,
                                                          New.Spearman.p.cp2 = 
                                                             test.performance$spearman.p,
                                                          stringsAsFactors = FALSE))
            }
            
            # Now get permutation-based feature importance for dataset1 (aligned to dataset2)
            if (run < 6 & ! flag.down.sample) {
              # Get permutation-based feature importance
              feat.importance.Perm <- data.frame(importance_pvalues(final.model$fit$fit$fit, method = "altmann", formula = Metabolite ~ ., data = dt1.ds))
              names(feat.importance.Perm) <- c("Score","P.Value")
              feat.importance.Perm$Feature <- rownames(feat.importance.Perm)
              rownames(feat.importance.Perm) <- NULL
              feat.importance.Perm <- feat.importance.Perm %>% arrange(-Score)
              feat.importance.Perm$Dataset1 <- dataset1
              feat.importance.Perm$Dataset.Aligned.To <- dataset2
              feat.importance.Perm$Compound.Name <- tmp.metab.name
              feat.importance.Perm$HMDB <- tmp.hmdb
              feat.importance.Perm$Run <- run
              feat.importance.Perm$Re.normalized <- flag.re.normalize
              feat.importance.Perm$Down.sampled <- flag.down.sample
              
              # Update table with summarized feature importance adjusted to shared features
              metab.contributors2 <- bind_rows(metab.contributors2, feat.importance.Perm)
            }
          } # Done iterating over runs 
        } # Done iterating over dataset2
      } # Done iterating over dataset1
    }
  }
} # Done iterating over metabolites

write_delim(cross.pred.results2, 
            path = '../data/Pairwise_Cross_Pred_Results.tsv',
            delim = "\t")
write_delim(metab.contributors2, 
            path = '../data/Pairwise_Contrib_Results.tsv',
            delim = "\t")

# Clean up
rm(tmp.hmdb, tmp.metab.name, relevant.datasets, flag.re.normalize, flag.down.sample, 
   i, j, dataset1, dataset2, raw.metab.name1, raw.metab.name2, genera1, genera2, 
   shared.genera.feats, dt1, dt2, n1, n2, run, 
   metab.vector, metab.vector.scaled, n.min, dt1.ds, dt2.ds,
   model, workflow.obj, hyparam.grid, loocv.preds, loocv.results,
   final.model, test.preds, test.performance, prev.loocv.results1, prev.loocv.results2,
   rf.explainer, rf.vip, rf.vip.sum)
```

#### Organize results

```{r LOAD_PAIRWISE_H_CONTRIBS_CROSS_PREDS, warning=FALSE, message=FALSE}
pairwise.comp.cross.pred <- read_delim("../data/Pairwise_Cross_Pred_Results.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

# (Without downsampling)
pairwise.comp.contribs <- read_delim("../data/Pairwise_Contrib_Results.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

# If results include datasets no longer relevant, remove them now
pairwise.comp.cross.pred <- pairwise.comp.cross.pred %>%
  filter(Dataset1 %in% datasets.to.train) %>%
  filter(Dataset2 %in% datasets.to.train) 

pairwise.comp.contribs <- pairwise.comp.contribs %>%
  filter(Dataset1 %in% datasets.to.train) %>%
  filter(Dataset.Aligned.To %in% datasets.to.train) 

# Take only universal + >3 well-performing models (redundant)
pairwise.comp.cross.pred <- pairwise.comp.cross.pred %>%
  filter(HMDB %in% metabs.for.contrib.analysis) 
pairwise.comp.contribs <- pairwise.comp.contribs %>%
  filter(HMDB %in% metabs.for.contrib.analysis) 

# Use only down-sampled version for cross-preds analysis
pairwise.comp.cross.pred <- pairwise.comp.cross.pred %>%
  filter(Down.sampled)

# Sanity / basic stats
print(paste(length(unique(pairwise.comp.cross.pred$HMDB)), "metabolites were analyzed for pairwise comparisons of models"))
```

Average over runs.

```{r}
pairwise.comp.contribs <- pairwise.comp.contribs %>%
  group_by(HMDB, Compound.Name, 
           Re.normalized, Down.sampled, 
           Dataset1, Dataset.Aligned.To, Feature) %>%
  summarise(Mean.Score = mean(Score), 
            Mean.P = mean(P.Value),
            .groups = "drop") %>%
  group_by(HMDB, Compound.Name, 
           Re.normalized, Down.sampled, 
           Dataset1, Dataset.Aligned.To) %>%
  mutate(Feat.Rank = rank(-Mean.Score)) 
```

```{r}
pairwise.comp.cross.pred <- pairwise.comp.cross.pred %>%
  group_by(HMDB, Compound.Name, 
           Re.normalized, Down.sampled, 
           Dataset1, Dataset2, 
           N.features1, N.features2, 
           N.shared.features, N.samples1, N.samples2,
           Prev.Spearman.rho.cv1, Prev.Spearman.fdr.cv1, 
           Prev.Spearman.rho.cv2, Prev.Spearman.fdr.cv2) %>%
  summarise(N.sanity.5 = n(),
            New.Spearman.rho.cv1 = mean(New.Spearman.rho.cv1),
            New.Spearman.p.cv1 = mean(New.Spearman.p.cv1),
            New.Spearman.rho.cp2 = mean(New.Spearman.rho.cp2),
            New.Spearman.p.cp2 = mean(New.Spearman.p.cp2),
            .groups = "drop") 

# Now also add FDR corrections, at the dataset level (so over all metabolites and paired datasets)
pairwise.comp.cross.pred <- pairwise.comp.cross.pred %>%
  group_by(Re.normalized, Down.sampled, Dataset1) %>%
  mutate(New.Spearman.fdr.cv1 = p.adjust(New.Spearman.p.cv1, method="fdr")) %>%
  ungroup() %>%
  group_by(Re.normalized, Down.sampled, Dataset2) %>%
  mutate(New.Spearman.fdr.cp2 = p.adjust(New.Spearman.p.cp2, method="fdr")) %>%
  ungroup()
```


#### For down-sampled models: filter only those still well-performing

For the cross-predictability analysis we discard models that after down-sampling did no longer perform well, using our "well-predicted" definition.  

```{r}
tmp <- pairwise.comp.cross.pred %>%
  filter(New.Spearman.rho.cv1 > rho.threshold & New.Spearman.fdr.cv1 <= fdr.threshold)

print(paste(nrow(tmp),"of",
            nrow(pairwise.comp.cross.pred),
            "models trained are still well-performing after (possible) down-sampling"))
print(paste("Overall",n_distinct(tmp$HMDB),"of",
            n_distinct(pairwise.comp.cross.pred$HMDB),
            "metabolites are included in downstream analyses"))

pairwise.comp.cross.pred <- tmp
rm(tmp)
```

#### Define well-transfered models

```{r}
# We add a flag column with indication of "Well.Transferred"
pairwise.comp.cross.pred <- pairwise.comp.cross.pred %>%
  mutate(Well.Transferred = (New.Spearman.rho.cp2 > rho.threshold & New.Spearman.fdr.cp2 <= fdr.threshold))

print(paste(sum(pairwise.comp.cross.pred$Well.Transferred),"of",
            nrow(pairwise.comp.cross.pred),
            "models were well-transfered from one dataset to another"))

# We create another table version where we summarize cross-predictability results per dataset pair
# (each pair appears twice, in both train-test directions)
pairwise.comp.cross.pred2 <- pairwise.comp.cross.pred %>%
  select(HMDB, Dataset1, Dataset2, Well.Transferred)
pairwise.comp.cross.pred2 <- bind_rows(pairwise.comp.cross.pred2 %>%
                                         mutate(Flipped = F),
                                       # Flip direction
                                       pairwise.comp.cross.pred2 %>%
                                         rename(DatasetT = Dataset1) %>%
                                         rename(Dataset1 = Dataset2) %>%
                                         rename(Dataset2 = DatasetT) %>%
                                         mutate(Flipped = T))
pairwise.comp.cross.pred2 <- pairwise.comp.cross.pred2 %>%
  group_by(HMDB, Dataset1, Dataset2) %>%
  summarise(N.Rows = n(), 
            N.Well.Transferred = sum(Well.Transferred), # 2 = well-transferred in both directions, 1 = one direction
            .groups = "drop") 
```

### Organize pairwise dataset comparisons

Here we collect several stats about how similar/consistent feature importances are, based on pairwise comparisons of models for each metabolite. These stats include:  

* Spearman correlation on importance scores (p values) of shared feature  
* Pearson correlation -"-  
* Fisher's exact test on significant features in each set (P<0.05/P<0.1)
* Number of features that are significant in both datasets (of shared features only, so that we don't "punish" for small overlaps of features)  

```{r CONTRIBS_COMPARISONS, warning=FALSE}
feat.importance.pairwise.comps <- data.frame(stringsAsFactors = F)
adj.feat.importance.pairwise.comps <- data.frame(stringsAsFactors = F)

for(tmp.hmdb in metabs.for.contrib.analysis) { 
  tmp.metab.name <- metabolite.stats$Compound.Name[metabolite.stats$HMDB == tmp.hmdb]
  
  # Organize contribution results for the specific metabolite (few different versions of the table)
  tmp <- get.hmdb.contribs(contribs, tmp.hmdb)
  contrib.hmdb.wide <- tmp$contrib.hmdb.wide
  ds.ordering <- unique(tmp$contrib.hmdb$Task.Dataset)
  
  # Analyze similarity between datasets using a few different methods
  feat.imp.comps <- get.feat.importance.comparisons(contrib.hmdb.wide, datasets.ordering = ds.ordering)
  feat.imp.comps$HMDB = tmp.hmdb
  feat.imp.comps$Compound.Name = tmp.metab.name
  feat.importance.pairwise.comps <- bind_rows(feat.importance.pairwise.comps, feat.imp.comps)
  
  # Same as above, but this time with "adjusted" models 
  #  (for each pairwise comparison, only features shared by both models were used for training and for feature importance calculations)
  adj.feat.imp.comps <- get.adj.feat.importance.comparisons(pairwise.comp.contribs, datasets.ordering = ds.ordering, hmdb = tmp.hmdb)
  adj.feat.imp.comps$HMDB = tmp.hmdb
  adj.feat.imp.comps$Compound.Name = tmp.metab.name
  adj.feat.importance.pairwise.comps <- bind_rows(adj.feat.importance.pairwise.comps, adj.feat.imp.comps)
}

feat.importance.pairwise.comps <- feat.importance.pairwise.comps %>%
  mutate(Pairwise.Adjusted = FALSE)
adj.feat.importance.pairwise.comps <- adj.feat.importance.pairwise.comps %>%
  mutate(Pairwise.Adjusted = TRUE)
feat.importance.pairwise.comps <- bind_rows(feat.importance.pairwise.comps, adj.feat.importance.pairwise.comps)

# Clean up
rm(tmp.hmdb, tmp.metab.name, tmp, contrib.hmdb.wide, feat.imp.comps, adj.feat.imp.comps, adj.feat.importance.pairwise.comps)
```

### Summarize at metabolite level

```{r}
# Aggregate stats per metabolite (and per adjusted/non-adjusted models)
feat.importance.stats <- feat.importance.pairwise.comps %>%
  group_by(HMDB, Compound.Name, Pairwise.Adjusted) %>%
  # (WP = weighted pearson)
  summarise(N.pairwise.comparisons = n(),
            Spearman.Cor.Min = min(Cor.Spearman),
            Spearman.Cor.Max = max(Cor.Spearman),
            Spearman.Cor.Med = median(Cor.Spearman),
            Spearman.Cor.Avg = mean(Cor.Spearman),
            Spearman.Cor.Std = sd(Cor.Spearman),
            Spearman.Cor.Signif = sum(Cor.Spearman > 0 & Cor.Spearman.P <= 0.1),
            OR.0.1.Signif = sum(Fisher.0.1.OR > 1 & Fisher.0.1.P <= 0.1),
            OR.0.05.Signif = sum(Fisher.0.05.OR > 1 & Fisher.0.05.P <= 0.1),
            Shared.Feats.0.1.Min = min(Shared.Feats.0.1),
            Shared.Feats.0.1.Max = max(Shared.Feats.0.1),
            Shared.Feats.0.1.Med = median(Shared.Feats.0.1),
            Shared.Feats.0.1.Avg = mean(Shared.Feats.0.1),
            Shared.Feats.0.1.Std = sd(Shared.Feats.0.1),
            Shared.Feats.0.05.Min = min(Shared.Feats.0.05),
            Shared.Feats.0.05.Max = max(Shared.Feats.0.05),
            Shared.Feats.0.05.Med = median(Shared.Feats.0.05),
            Shared.Feats.0.05.Avg = mean(Shared.Feats.0.05),
            Shared.Feats.0.05.Std = sd(Shared.Feats.0.05),
            Shared.Feats.Avg = mean(N.Feat.In.Common),
            Shared.Feats.Std = sd(N.Feat.In.Common),
            .groups="drop") 

tmp.mtb.ordering <- feat.importance.stats %>%
  filter(! Pairwise.Adjusted) %>%
  arrange(Spearman.Cor.Med) %>%
  pull(Compound.Name)

feat.importance.stats <- feat.importance.stats %>%
  mutate(Compound.Name = factor(Compound.Name, levels = tmp.mtb.ordering))

# Add stats about known contributing genera
# feat.importance.stats <- feat.importance.stats %>%
#   left_join(contributors.counts %>% select(-Compound.Name), by = "HMDB")

rm(tmp.mtb.ordering)
```

### Plot - contributors comparison overview

Plot statistics of shared significant and overall features:

```{r fig.width=9, fig.height=3.6}
category1 <- "Average no. of shared features\n(over dataset pairs)"
category2 <- "Average no. of significant contributors\n(over single datasets)"
category3 <- "Average no. of shared significant contributors\n(over dataset pairs)"

dt.category1 <- feat.importance.stats %>%
    filter(! Pairwise.Adjusted) %>%
    select(Compound.Name, N.pairwise.comparisons, 
           Shared.Feats.Avg, Shared.Feats.Std) %>%
    mutate(Category = category1) %>%
    rename(Average = Shared.Feats.Avg, Std = Shared.Feats.Std) 

dt.category2 <- contribs %>% 
    filter(Task.HMDB %in% feat.importance.stats$HMDB) %>%
    group_by(Task.Dataset, Compound.Name) %>% 
    summarise(Significant.Contributors.0.1 = sum(Mean.P<0.1), .groups="drop") %>%
    group_by(Compound.Name) %>%
    summarise(Average = mean(Significant.Contributors.0.1), 
              Std = sd(Significant.Contributors.0.1), .groups="drop") %>%
    mutate(Category = category2)

dt.category3 <- feat.importance.stats %>%
    filter(! Pairwise.Adjusted) %>%
    select(Compound.Name, N.pairwise.comparisons, 
           Shared.Feats.0.1.Avg, Shared.Feats.0.1.Std) %>%
    mutate(Category = category3) %>%
    rename(Average = Shared.Feats.0.1.Avg, Std = Shared.Feats.0.1.Std) %>%
  arrange(-Average)

tmp <- bind_rows(dt.category1, dt.category2, dt.category3)
tmp$Category <- factor(tmp$Category, levels = c(category1, category2, category3))
tmp$Compound.Name <- factor(tmp$Compound.Name, levels = dt.category3$Compound.Name)

# Plot customizations
cols.map <- c("grey","#0f4c5c","#C87EAC")
names(cols.map) <- c(category1, category2, category3)
point.sizes.map <- c(2,2,5.5)
names(point.sizes.map) <- c(category1, category2, category3)

ggplot(tmp, aes(x = Compound.Name, y = Average, color = Category)) +
  geom_errorbar(aes(ymin = Average-Std, ymax = Average+Std),
                position = position_dodge(0.3), 
                width = 0.7) +
  geom_point(aes(size = Category), position = position_dodge(0.3)) +
  scale_size_manual(values = point.sizes.map) +
  scale_color_manual(values = cols.map) +
  geom_text(data = dt.category3, 
            mapping = aes(label = round(Average,1), 
                          x = Compound.Name, y = Average),
            size = 2.4, color = "black", nudge_x = 0.1, nudge_y = 0.1) +
  xlab(NULL) +
  ylab("Number of features") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, size = 8, hjust = 1, vjust = 0.3),
        legend.position = "top", 
        legend.title = element_blank(),
        legend.text = element_text(size = 8),
        legend.box.margin=margin(-10,-10,-10,-10)
        ) 

rm(tmp, category1, category2, category3, dt.category1, dt.category2, dt.category3)
```

### Stats 

```{r}
n.agree.at.least.once <- nrow(feat.importance.stats %>% 
                                filter(! Pairwise.Adjusted) %>% 
                                filter(Spearman.Cor.Signif > 0))
print(paste(n.agree.at.least.once,
            "of", nrow(feat.importance.stats %>% 
                         filter(! Pairwise.Adjusted)),
            "metabolites exhibit a significant Spearman correlation between feature P values of different datasets at least once"))

print(paste("Similarily, using pairwise-adjusted models, we get",
           nrow(feat.importance.stats %>% filter(Pairwise.Adjusted) %>% filter(Spearman.Cor.Signif > 0)), 
           "metabolites"))

print("Metabolites vary in how well do genera feature P values correlate between any pair of datasets, on average. Examples of low avergae correlation and high average correlation:")

feat.importance.stats %>%
  filter(! Pairwise.Adjusted) %>%
  select(Compound.Name, Pairwise.Adjusted, 
         N.pairwise.comparisons, 
         Spearman.Cor.Avg, Spearman.Cor.Std, 
         Shared.Feats.0.1.Avg, Shared.Feats.0.1.Std) %>%
  filter(Spearman.Cor.Avg <= 0.05, Spearman.Cor.Std < 0.2) %>%
  kable() %>%
  kable_styling(font_size = 9) %>%
  column_spec(1, background = "orange")

feat.importance.stats %>%
  filter(! Pairwise.Adjusted) %>%
  select(Compound.Name, Pairwise.Adjusted, 
         N.pairwise.comparisons, 
         Spearman.Cor.Avg, Spearman.Cor.Std, 
         Shared.Feats.0.1.Avg, Shared.Feats.0.1.Std) %>%
  filter(Spearman.Cor.Avg >= 0.25) %>%
  kable() %>%
  kable_styling(font_size = 9) %>%
  column_spec(1, background = "orange")

rm(n.agree.at.least.once)
```

### Plot per metabolite

We plot heatmaps of feature importance scores per metabolite, pairwise pearson correlations between feature importance scores (p values), and a matrix of cross-prediction results (where the model was trained on the column-dataset and tested on the row-dataset).   

In the cross-prediction matrix, empty cells mean that the trained model after downsampling did not perform well.  

```{r CONTRIBS_COMPARISONS_VIZ, fig.width=6.9, fig.height=3.4}
all.cp.plots <- list()
all.fic.plots <- list()

for(tmp.metab.name in levels(feat.importance.stats$Compound.Name)) { # tmp.metab.name = "Taurine"
  tmp.hmdb <- metabolite.stats$HMDB[metabolite.stats$Compound.Name == tmp.metab.name]
  
  # Get list of features to include in plot
  features.to.include <- get.features.to.include.in.plot(contribs, tmp.hmdb)
  
  # Organize contribution results for the specific metabolite 
  tmp <- get.hmdb.contribs(contribs, tmp.hmdb)
  contrib.hmdb <- tmp$contrib.hmdb
  ds.ordering <- unique(contrib.hmdb$Task.Dataset)
  contrib.hmdb <- contrib.hmdb %>% 
    mutate(Task.Dataset = factor(Task.Dataset, levels = ds.ordering,
                                 labels = substr(ds.ordering,1,2)))
  
  # Order features as above (TODO: can be more elegant)
  tmp.order <- match(levels(contrib.hmdb$Feature), features.to.include)
  tmp.order <- tmp.order[!is.na(tmp.order)]
  features.to.include <- features.to.include[tmp.order]
  
  # Get main heatmap plot
  p1 <- plot.ggplot.heatmap.feat.imp2(contrib.hmdb, features.to.include)
  #p1 <- p1 + theme(plot.margin = unit(c(5.5, 0, main.marg.bottom, 5.5), "points"))
  
  # Plot summarized statistics - pearson comparisons and cross-predictability
  feat.imp.comps <- feat.importance.pairwise.comps %>%
    filter(HMDB == tmp.hmdb) %>%
    filter(! Pairwise.Adjusted) %>%
    mutate(Dataset1 = factor(Dataset1, levels = ds.ordering)) %>%
    mutate(Dataset2 = factor(Dataset2, levels = ds.ordering)) %>%
    select(Dataset1, Dataset2, Cor.Spearman, Cor.Spearman.P, Compound.Name) %>%
    mutate(tmp.FDR = p.adjust(Cor.Spearman.P, method = "fdr")) %>%
    mutate(Significance = get.signif.marks(tmp.FDR)) 
  
  # Plot summarized statistics - cross-predictability
  tmp <- pairwise.comp.cross.pred %>%
    filter(HMDB == tmp.hmdb) %>%
    mutate(Dataset1 = factor(Dataset1, levels = ds.ordering,
                             labels = substr(ds.ordering,1,2))) %>%
    mutate(Dataset2 = factor(Dataset2, levels = ds.ordering,
                             labels = substr(ds.ordering,1,2))) %>% 
    tidyr::complete(Dataset1, Dataset2) %>%
    mutate(Significance.cp2 = get.signif.marks(New.Spearman.fdr.cp2)) %>%
    select(Dataset1, Dataset2, 
           New.Spearman.rho.cp2, New.Spearman.fdr.cp2, 
           Significance.cp2, Well.Transferred) %>%
    mutate(p.label.cp = ifelse(is.na(New.Spearman.rho.cp2), 
                               "", 
                               paste("CP:\n",round(New.Spearman.rho.cp2,2), 
                                     Significance.cp2)))  # CP = Cross-predictability
    
  p3 <- ggplot(tmp, aes(x = Dataset1, y = Dataset2, fill = Well.Transferred)) + 
    geom_tile(color = "black") + 
    geom_text(aes(label=p.label.cp), size = 2) +
    geom_abline(slope = -1, intercept = length(ds.ordering)+1) +
    xlab("Trained on") + 
    ylab("Tested on") + 
    theme_classic() + 
    scale_y_discrete(position = "right") +
    scale_x_discrete(limits = rev(substr(ds.ordering,1,2))) +
    scale_fill_manual(values = c("TRUE" = "#C52E2B", "FALSE" = "gray88")) +
    ggtitle(tmp.metab.name) + 
    theme(axis.text.x = element_text(angle = 90, size = 9, 
                                     hjust = 1, vjust = 0.3), 
          axis.text.y = element_text(size = 9),
          axis.title = element_text(size = 9),
          legend.position = "none", #"top",
          #legend.text = element_text(size=6),
          plot.title = element_text(hjust=0.5, size = 10),
          plot.margin = unit(c(0,0,0.1,-0.07), "cm"))
  all.cp.plots[[tmp.metab.name]] <- p3 
  
  # Plot summarized statistics - correlation between feature p values
  tmp <- feat.imp.comps %>% 
    select(Dataset1, Dataset2, Cor.Spearman, Significance) %>%
    mutate(Dataset1 = factor(Dataset1, levels = ds.ordering,
                             labels = substr(ds.ordering,1,2))) %>%
    mutate(Dataset2 = factor(Dataset2, levels = ds.ordering,
                             labels = substr(ds.ordering,1,2))) %>% 
    tidyr::complete(Dataset2) %>%
    mutate(p.label.fic = ifelse(is.na(Cor.Spearman), 
                                "", 
                                paste("FIC:\n",
                                      round(Cor.Spearman, 2), 
                                      Significance))) # FIC = Feature Importance Correlation
  
  p4 <- ggplot(tmp, aes(x = Dataset1, y = Dataset2, 
                        fill = (Significance==""))) + 
    geom_tile(color = "black") + 
    geom_text(aes(label=p.label.fic), size = 2) +
    geom_abline(slope = -1, intercept = length(ds.ordering)+1) +
    xlab("Dataset 1") + 
    ylab("Dataset 2") + 
    theme_classic() + 
    scale_y_discrete(position = "right") +
    scale_x_discrete(limits = rev(substr(ds.ordering,1,2))) +
    scale_fill_manual(values = c("FALSE" = "#C52E2B", "TRUE" = "gray88")) +
    ggtitle(tmp.metab.name) + 
    theme(axis.text.x = element_text(angle = 90, size = 9, 
                                     hjust = 1, vjust = 0.3), 
          axis.text.y = element_text(size = 9),
          legend.position = "none", #"top",
          #legend.text = element_text(size=6),
          axis.title = element_text(size = 9),
          plot.title = element_text(hjust=0.5, size = 10),
          plot.margin = unit(c(0,0,0.1,-0.07), "cm"))
  all.fic.plots[[tmp.metab.name]] <- p4 
  
  print(p1 + ggtitle(tmp.metab.name))
  #grid.arrange(p1 + ggtitle(""), p3, widths = c(0.65, 0.35), 
  #             top = paste(tmp.hmdb, "~", tmp.metab.name))
}

rm(tmp.hmdb, tmp.metab.name, features.to.include, 
   tmp, contrib.hmdb, tmp.order, feat.imp.comps, p1, p3, p4)
```

#### CP plots

```{r fig.width=13, fig.height=17}
do.call("grid.arrange", c(all.cp.plots, ncol=6))
```

#### FIC plots

```{r fig.width=13, fig.height=17, warning=FALSE}
do.call("grid.arrange", c(all.fic.plots, ncol=6))
```

### Consistent genera contributors 

We next generate, per metabolite, a list of genera that were consistently significantly important (P<0.1) across all models in which they appeared.

```{r}
consistent.contributors <- contribs %>%
  filter(Healthy) %>%
  group_by(Compound.Name, Task.HMDB) %>%
  mutate(N.Models = n_distinct(Task.Dataset)) %>%
  filter(N.Models >= 3) %>%
  group_by(Compound.Name, Task.HMDB, N.Models, Feature) %>%
  summarise(N.models.including.genus = n(),
            N.models.in.which.P.below.0.1 = 
              paste0(sum(Mean.P < 0.1),' [',round(100*sum(Mean.P < 0.1)/n(),1),'%]'),
            Perc.P.0.1 = 100*sum(Mean.P < 0.1)/n(),
            .groups = "drop") %>%
  filter(N.models.including.genus >= 3) %>%
  filter(Perc.P.0.1 > 50) %>%
  rename(HMDB = Task.HMDB) %>%
  arrange(Compound.Name)

print(paste("We identify",nrow(consistent.contributors),
            "consistent contributors, to",n_distinct(consistent.contributors$HMDB),"distinct metabolites"))

consistent.contributors %>%
  select(-Perc.P.0.1, -HMDB, -N.Models) %>%
  kable() %>%
  kable_styling(font_size = 10)
```

## 11. Healthy vs. disease

### Get cross-predictability

Now, for each relevant study we check how well does a model trained on healthy subjects predict metabolite levels in disease. We record these cross-predictability results.    
We run this for universal metabolites only. 

```{r PAIRWISE_HC_DISEASE, warning = FALSE, eval = FALSE}
cross.pred.results3 <- data.frame(stringsAsFactors = F)
genera.trans.choice <- "RELATIVE"
model.choice <- "RF"

# Iterate over relevant studies
for (dataset.orig in c("FRANZOSA_IBD", "KIM_ADENOMAS", "SINHA_CRC", "YACHIDA_CRC", "iHMP_IBD")) { 
  print(paste("Working on dataset:", dataset.orig))
  print(format(Sys.time(), format="%H:%M:%S"))
  dataset.h.full <- paste(dataset.orig, "HEALTHY", sep = "_")
  dataset.d.full <- paste(dataset.orig, "DISEASE", sep = "_")
      
  # Get list of metabolites to work on + previous performance results 
  #  (metabolites well predicted in healthy according to previous analysis)
  prev.loocv.results <- metab.pred.results.all %>%
    filter(grepl(dataset.orig, Task.Dataset)) %>%
    group_by(Task.Metabolite.Raw) %>%
    filter(n() == 2) %>%
    ungroup() %>%
    filter(Task.HMDB %in% REM.universal$HMDB) %>%
    inner_join(metab.pred.results.sum %>%
                 filter(grepl(dataset.orig, Task.Dataset)) %>%
                 filter(Avg.Spearman >= rho.threshold.on.loocv) %>%
                 filter(Avg.Spearman.FDR <= p.threshold.on.loocv) %>%
                 select(Task.Metabolite.Raw, Task.HMDB, Compound.Name, Pipe.ID),
               by = c("Task.Metabolite.Raw","Task.HMDB", "Compound.Name", "Pipe.ID"))
  
  metabs.for.training <- prev.loocv.results %>%
    select(Task.HMDB, Task.Metabolite.Raw) %>%
    distinct()
  print(paste(nrow(metabs.for.training), "metabolites in this dataset will be analyzed.")) 
  
  if (nrow(metabs.for.training) == 0) {next}

   # Prepare feature table (genera abundances)
  genera.full <- get.feature.table(dataset.orig, 
                                   genera.trans, 
                                   genera.trans.choice)
  
  # Iterate over metabolites
  for (i in 1:nrow(metabs.for.training)) { # i = 3
    cat('.')
    # Get metabolite name
    metabolite <- metabs.for.training$Task.Metabolite.Raw[i]
    metabolite.hmdb <- metabs.for.training$Task.HMDB[i]
    
    # Add target variable (metabolite values) to feature table (for training)
    ## Patch: for some reason some trailing white-spaces have been removed in some versions of metabolite name, so we optionally add it back if needed
    if (! metabolite %in% rownames(mtb.trans$LOG[[dataset.orig]])) { 
      metabolite2 <- paste0(metabolite, ' ') 
    } else { metabolite2 <- metabolite }
    metab.vector <- mtb.trans$LOG[[dataset.orig]][metabolite2, rownames(genera.full$dt)]
    # Scale (zero-mean unit-variance)
    metab.vector.scaled <- c(scale(metab.vector))
    dt.full <- cbind(genera.full$dt, Metabolite = metab.vector.scaled)
  
    # Get samples for train (healthy) and test (disease)
    samples.for.training <- 
      colnames(genera.trans[[genera.trans.choice]][[dataset.h.full]])
    samples.for.testing <- 
      colnames(genera.trans[[genera.trans.choice]][[dataset.d.full]])
    dt.train <- dt.full[samples.for.training,]
    dt.test <- dt.full[samples.for.testing,]
    n.train <- length(samples.for.training)
    n.test <- length(samples.for.testing)
            
    # For longitudinal datasets, get a named vector that will serve as a map between sample names to subject names
    # If the dataset is not longitudinal, the map is from sample name to sample name. (so it can be effectively treated the same)
    sample.to.subject <- get.sample.to.subject(dataset.orig, metadatas, mtb.trans)
    
    # Run a few times to verify stability
    for (run in 1:10) { # run = 1
    
      # Subsample larger study group to match size of smaller study group
      if (n.train > n.test) {
        dt.train2 <- dt.train[sample(x = n.train, n.test),]
        dt.test2 <- dt.test
      } else if (n.train < n.test) {
        dt.train2 <- dt.train
        dt.test2 <- dt.test[sample(x = n.test, n.train),]
      }
            
      # Update mappings of sample to subject
      tmp.sample.to.subject.h <- sample.to.subject[names(sample.to.subject) %in% rownames(dt.train2)]
      tmp.sample.to.subject.d <- sample.to.subject[names(sample.to.subject) %in% rownames(dt.test2)]
      tmp.unique.subjects.h <- unique(unname(tmp.sample.to.subject.h))
      tmp.unique.subjects.d <- unique(unname(tmp.sample.to.subject.d))
            
      # Train predictor for healthy dataset (we train from scratch for cases where down-sampling was performed)
      model <- my.get.model(model.choice)
      workflow.obj <- my.get.workflow(model)
      hyparam.grid <- my.get.hyperparameter.grid(model.choice, n.features = genera.full$n, dummy = TRUE)
      
      # Collect CV performance metrics - healthy
      loocv.preds <- foreach(ii = 1:length(tmp.unique.subjects.h), 
                                   .combine = rbind, 
                                   .packages = c('tidymodels')) %dopar% { # ii=5
        tmp.samples.to.test.on <- names(tmp.sample.to.subject.h)[tmp.sample.to.subject.h == tmp.unique.subjects.h[ii]]
        tmp.trained.model <- workflow.obj %>% 
          finalize_workflow(parameters = hyparam.grid) %>% 
          fit(data = dt.train2[! rownames(dt.train2) %in% tmp.samples.to.test.on,])
        my.predict(tmp.trained.model, dt.train2[sample(tmp.samples.to.test.on, 1),])
      }
      loocv.results <- my.get.metrics(loocv.preds)
      names(loocv.results) <- paste0("loocv.", names(loocv.results))

      # Now train final model using entire train dataset
      final.model <- 
        workflow.obj %>% 
        finalize_workflow(parameters = hyparam.grid) %>% 
        fit(data = dt.train2)
      
      # Get predictions on test data (dataset2)
      test.preds <- my.predict(final.model, dt.test2)
      
      # Calculate test performance (for longitudinal datasets, we pick here one sample per subject)
      tmp.sample.to.subject.d2 <- sample(tmp.sample.to.subject.d) # Shuffle
      tmp.sample.to.subject.d2 <- tmp.sample.to.subject.d2[! duplicated(tmp.sample.to.subject.d2)] # Take first sample per subject
      test.preds <- test.preds[names(tmp.sample.to.subject.d2),]
      test.performance <- my.get.metrics(test.preds)
      
      # Get performance loocv performance for disease dataset (in cases downsampling occured, these should be lower than previous results)
      # Collect CV performance metrics - healthy
      dis.loocv.preds <- foreach(ii = 1:length(tmp.unique.subjects.d), 
                                   .combine = rbind, 
                                   .packages = c('tidymodels')) %dopar% { # ii=5
        tmp.samples.to.test.on <- names(tmp.sample.to.subject.d)[tmp.sample.to.subject.d == tmp.unique.subjects.d[ii]]
        tmp.trained.model <- workflow.obj %>% 
          finalize_workflow(parameters = hyparam.grid) %>% 
          fit(data = dt.test2[! rownames(dt.test2) %in% tmp.samples.to.test.on,])
        my.predict(tmp.trained.model, dt.test2[sample(tmp.samples.to.test.on, 1),])
      }
      dis.loocv.results <- my.get.metrics(dis.loocv.preds)
      names(dis.loocv.results) <- paste0("loocv.", names(dis.loocv.results))
      
      # Finally, extract previous performance metrics on healthy+disease set using cv (for later sanity)
      prev.loocv.results.h <- prev.loocv.results %>%
        filter(Task.Dataset == dataset.h.full & 
                 Task.Metabolite.Raw == metabolite)  
      prev.loocv.results.d <- prev.loocv.results %>%
        filter(Task.Dataset == dataset.d.full & 
                 Task.Metabolite.Raw == metabolite)
      
      cross.pred.results3 <- bind_rows(cross.pred.results3,
                                       data.frame(Original.Dataset = dataset.orig,
                                                  Compound.Name = prev.loocv.results.h$Compound.Name,
                                                  HMDB = metabolite.hmdb,
                                                  Raw.Metabolite.Name = metabolite,
                                                  # Numbers before down-sampling
                                                  N.Samples.Healthy = n.train,
                                                  N.Samples.Disease = n.test,
                                                  N.Unique.Subjects1.DS = n_distinct(tmp.sample.to.subject.h),
                                                  N.Unique.Subjects2.DS = n_distinct(tmp.sample.to.subject.d2),
                                                  Run = run,
                                                  # Previous results
                                                  Prev.Spearman.rho.CV.Healthy = 
                                                     prev.loocv.results.h$Avg.Spearman,
                                                  Prev.Spearman.fdr.CV.Healthy = 
                                                     prev.loocv.results.h$Avg.Spearman.FDR,
                                                  Prev.Spearman.rho.CV.Disease = 
                                                     prev.loocv.results.d$Avg.Spearman,
                                                  Prev.Spearman.fdr.CV.Disease = 
                                                     prev.loocv.results.d$Avg.Spearman.FDR,
                                                  # New results
                                                  New.Spearman.rho.CV.Healthy = 
                                                     loocv.results$loocv.spearman.rho,
                                                  New.Spearman.p.CV.Healthy = 
                                                     loocv.results$loocv.spearman.p,
                                                  New.Spearman.rho.CV.Disease = 
                                                     dis.loocv.results$loocv.spearman.rho,
                                                  New.Spearman.p.CV.Disease = 
                                                     dis.loocv.results$loocv.spearman.p,
                                                  New.Spearman.rho.Cross.Pred = 
                                                     test.performance$spearman.rho,
                                                  New.Spearman.p.Cross.Pred = 
                                                     test.performance$spearman.p,
                                                  stringsAsFactors = FALSE))

    } # Done iterating over runs
  } # Done iterating over metabolites
} # Done iterating over datasets

# Save
write_delim(cross.pred.results3, 
            path = '../data/Pairwise_Cross_Pred_Results_Healthy_Disease.tsv',
            delim = "\t")

# Clean up
rm(dataset.orig, dataset.h.full, dataset.d.full, prev.loocv.results, metabs.for.training, i, metabolite, 
   metabolite.hmdb, metabolite2, metab.vector, metab.vector.scaled, dt.full, samples.for.training, 
   samples.for.testing, dt.train, dt.test, n.train, n.test, sample.to.subject, run, dt.train2, dt.test2, 
   tmp.sample.to.subject.h, tmp.sample.to.subject.d, tmp.unique.subjects.h, model, workflow.obj, hyparam.grid, 
   loocv.preds, ii, tmp.samples.to.test.on, tmp.trained.model, loocv.results, final.model, test.preds, 
   test.performance, prev.loocv.results.h, prev.loocv.results.d)
```

### Organize results

Load results, verify the number of analyzed metabolites is as expected.   
We remove all cases where the healthy model was no longer well-predicting the metabolite (this happened in cases where the control group was larger than the case group and was therefor downsampled).

```{r LOAD_PAIRWISE_H_VS_D_CONTRIBS_CROSS_PREDS, warning=FALSE, message=FALSE}
pairwise.cp.h.vs.d.raw <- read_delim("../data/Pairwise_Cross_Pred_Results_Healthy_Disease.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

pairwise.cp.h.vs.d <- pairwise.cp.h.vs.d.raw %>%
  group_by(Original.Dataset, 
           Compound.Name, HMDB, 
           N.Samples.Healthy, N.Samples.Disease,
           Prev.Spearman.rho.CV.Healthy, Prev.Spearman.fdr.CV.Healthy, Prev.Spearman.rho.CV.Disease, Prev.Spearman.fdr.CV.Disease) %>%
  summarise(Spearman.rho.CV.Healthy = mean(New.Spearman.rho.CV.Healthy), 
            Spearman.p.CV.Healthy = mean(New.Spearman.p.CV.Healthy),
            Spearman.rho.CV.Disease = mean(New.Spearman.rho.CV.Disease), 
            Spearman.p.CV.Disease = mean(New.Spearman.p.CV.Disease),
            Spearman.rho.Cross.Pred = mean(New.Spearman.rho.Cross.Pred),
            Spearman.p.Cross.Pred = mean(New.Spearman.p.Cross.Pred),
            .groups="drop") %>%
  group_by(Original.Dataset) %>%
  mutate(Spearman.FDR.CV.Healthy = p.adjust(Spearman.p.CV.Healthy, 
                                            method = "fdr"),
         Spearman.FDR.Cross.Pred = p.adjust(Spearman.p.Cross.Pred, 
                                            method = "fdr"),
         Spearman.FDR.CV.Disease = p.adjust(Spearman.p.CV.Disease, 
                                            method = "fdr")) %>%
  ungroup() %>%
  filter(Spearman.FDR.CV.Healthy < fdr.threshold & 
           Spearman.rho.CV.Healthy > rho.threshold.on.loocv) 

# Sanity / basic stats
print(paste(nrow(pairwise.cp.h.vs.d), "metabolites were analyzed (", n_distinct(pairwise.cp.h.vs.d$HMDB),
            "distinct ) for healthy-to-disease transferability, in", n_distinct(pairwise.cp.h.vs.d$Original.Dataset), "datasets"))
print("These are the number of metabolites analyzed per dataset:")
print(table(pairwise.cp.h.vs.d$Original.Dataset))
```


### Visualize cross-predictability

Per metabolite we visualize all the 2X2 matrices of performance results. (The plot is divided into 2).

```{r fig.width=7.6, fig.height=11}
# Organize data for plotting
tmp <- bind_rows(
  # 1 - H --> H
  pairwise.cp.h.vs.d %>%
    select(Original.Dataset, HMDB, Compound.Name, Prev.Spearman.rho.CV.Healthy, Prev.Spearman.fdr.CV.Healthy, 
           Spearman.rho.CV.Healthy, Spearman.FDR.CV.Healthy) %>%
    rename(Prev.Rho = Prev.Spearman.rho.CV.Healthy) %>%
    rename(Prev.FDR = Prev.Spearman.fdr.CV.Healthy) %>%
    rename(New.Rho = Spearman.rho.CV.Healthy) %>%
    rename(New.FDR = Spearman.FDR.CV.Healthy) %>%
    mutate(Trained.On = "HEALTHY", Tested.On = "HEALTHY"),
  # 2 - H --> D
  pairwise.cp.h.vs.d %>%
    select(Original.Dataset, HMDB, Compound.Name, Spearman.rho.Cross.Pred, Spearman.FDR.Cross.Pred) %>%
    mutate(Prev.Rho = NA) %>%
    mutate(Prev.FDR = NA) %>%
    rename(New.Rho = Spearman.rho.Cross.Pred) %>%
    rename(New.FDR = Spearman.FDR.Cross.Pred) %>%
    mutate(Trained.On = "HEALTHY", Tested.On = "DISEASE"),
  # 3 - D --> D
  pairwise.cp.h.vs.d %>%
    select(Original.Dataset, HMDB, Compound.Name, Prev.Spearman.rho.CV.Disease, Prev.Spearman.fdr.CV.Disease, 
           Spearman.rho.CV.Disease, Spearman.FDR.CV.Disease) %>%
    rename(Prev.Rho = Prev.Spearman.rho.CV.Disease) %>%
    rename(Prev.FDR = Prev.Spearman.fdr.CV.Disease) %>%
    rename(New.Rho = Spearman.rho.CV.Disease) %>%
    rename(New.FDR = Spearman.FDR.CV.Disease) %>%
    mutate(Trained.On = "DISEASE", Tested.On = "DISEASE"))

tmp <- tmp %>%
  mutate(Spearman.bin = cut(New.Rho, c(-1, 0.3, 0.5, 0.7, 1), right = F)) %>%
  mutate(Spearman.bin = plyr::revalue(Spearman.bin, 
                                      c("[-1,0.3)"="< 0.3", 
                                        "[0.7,1)"="> 0.7"))) %>%
  mutate(Tested.On = factor(Tested.On, levels = c("HEALTHY","DISEASE"), labels = c("H","D"))) %>%
  mutate(Trained.On = factor(Trained.On, levels = c("DISEASE","HEALTHY"), labels = c("D","H"))) %>%
  mutate(Significance = get.signif.marks(New.FDR)) %>%
  mutate(Color.Category = ifelse(New.FDR <= fdr.threshold & New.Rho >= rho.threshold, "Well-predicted", "Not well-predicted"))

# %>% mutate(Compound.Name2 = ifelse(HMDB %in% REM.universal$HMDB, paste(Compound.Name, "(U)"), Compound.Name))

# Only plot metabs that appear in 2 or more datasets
tmp2 <- tmp %>%
  group_by(HMDB) %>%
  filter(n_distinct(Original.Dataset) >= 2) %>%
  ungroup()

n.metabs <- n_distinct(tmp2$HMDB)
metabs.first.half <- unique(tmp2$HMDB)[1:ceiling(n.metabs/2)]

# Add newlines to some metabolite names just to get a more narrow plot
tmp2$Compound.Name[tmp2$Compound.Name == "2-Hydroxy-3-methylpentanoic acid"] <- "2-Hydroxy-3-methyl-\npentanoic acid"
tmp2$Compound.Name[tmp2$Compound.Name == "Asymmetric dimethylarginine"] <- "Asymmetric\ndimethylarginine"
tmp2$Compound.Name[tmp2$Compound.Name == "Docosapentaenoic acid (22n-6)"] <- "Docosapentaenoic\nacid (22n-6)"
tmp2$Compound.Name[tmp2$Compound.Name == "N6,N6,N6-Trimethyl-L-lysine"] <- "N6,N6,N6-Trimethyl-\nL-lysine"
#tmp2$Compound.Name[tmp2$Compound.Name == "xxx"] <- "xxx"

# Plot (1st half)
ggplot(tmp2 %>% filter(HMDB %in% metabs.first.half), aes(y = Trained.On, x = Tested.On)) +
  geom_tile(aes(fill = Color.Category), color = "black") +
  geom_text(aes(label = paste(round(New.Rho, 2), Significance)), size = 3) +
  scale_fill_manual("Spearman rho",
                    values = c("Not well-predicted" = "grey",
                               "Well-predicted" = "#EF3B2C"),
                    na.value="white") +
  theme_classic() +
  ylab("Group trained on") +
  xlab("Group tested on") +
  facet_grid(Compound.Name ~ Original.Dataset) +
  theme(strip.text.y = element_text(size = 8, angle = 0, hjust = 0),
        strip.text.x = element_text(size = 8),
        axis.text = element_text(size = 9),
        legend.title = element_blank())
```

```{r fig.width=6.5, fig.height=11}
# Plot (2nd half)
ggplot(tmp2 %>% filter(! HMDB %in% metabs.first.half), aes(y = Trained.On, x = Tested.On)) +
  geom_tile(aes(fill = Color.Category), color = "black") +
  geom_text(aes(label = paste(round(New.Rho, 2), Significance)), size = 3) +
  scale_fill_manual("Spearman rho",
                    values = c("Not well-predicted" = "grey",
                               "Well-predicted" = "#EF3B2C"),
                    na.value="white") +
  theme_classic() +
  ylab("Group trained on") +
  xlab("Group tested on") +
  facet_grid(Compound.Name ~ Original.Dataset) +
  theme(strip.text.y = element_text(size = 8, angle = 0, hjust = 0),
        strip.text.x = element_text(size = 8),
        axis.text = element_text(size = 9),
        legend.title = element_blank())

rm(tmp, tmp2, metabs.first.half, n.metabs)
```

### Contributors comparisons per metabolite

```{r CONTRIBS_COMPARISONS_VIZ_H_VS_D, fig.width=4.9, fig.height=2.7}
all.plots <- list()
i <- 0

for (tmp.metab.name in sort(unique(pairwise.cp.h.vs.d$Compound.Name))) { # tmp.metab.name = "Malonic acid"
  tmp.hmdb <- pairwise.cp.h.vs.d$HMDB[pairwise.cp.h.vs.d$Compound.Name == tmp.metab.name][1]
  tmp.datasets <- unique(pairwise.cp.h.vs.d %>% filter(Compound.Name == tmp.metab.name) %>% pull(Original.Dataset))
  
  for (dataset in tmp.datasets) { # dataset = "FRANZOSA_IBD"
    # Get list of features to include in plot
    features.to.include <- get.features.to.include.in.plot(contribs %>% 
                                                             filter(grepl(dataset, Task.Dataset)), 
                                                           tmp.hmdb, healthy.only = F)
    
    # Organize contribution results for the specific metabolite 
    tmp <- get.hmdb.contribs(contribs %>% filter(grepl(dataset, Task.Dataset)), tmp.hmdb, healthy.only = FALSE)
    contrib.hmdb <- tmp$contrib.hmdb
    contrib.hmdb <- contrib.hmdb %>% 
      mutate(Task.Dataset = factor(Task.Dataset, levels = c(paste0(dataset, "_DISEASE"),paste0(dataset, "_HEALTHY")),
                                 labels = c("DISEASE","HEALTHY")))
    
    if (n_distinct(contrib.hmdb$Task.Dataset) < 2) {print(paste("Skipping",tmp.metab.name,"in",dataset, "(was not well-predicted in disease)")); next}
    
    # Order features as above (TODO: can be more elegant)
    tmp.order <- match(levels(contrib.hmdb$Feature), features.to.include)
    tmp.order <- tmp.order[!is.na(tmp.order)]
    features.to.include <- features.to.include[tmp.order]
      
    # Get main heatmap plot
    p1 <- plot.ggplot.heatmap.feat.imp2(contrib.hmdb, features.to.include)
    p1 <- p1 + 
      ggtitle(paste0(tmp.metab.name, " [Dataset: ", dataset, "]")) +
      theme(plot.title = element_text(size = 11))
    # g1 <- ggplot_gtable(data = ggplot_build(plot = p1))
    # grid.newpage()
    # grid.draw(g1)
    # print(p1)
    i <- i+1
    all.plots[[i]] <- p1
  }
} 

all.plots.2 <- align_plots(plotlist = all.plots, align="h")

for (i in 1:length(all.plots.2)) { print(ggdraw(all.plots.2[[i]])) }

rm(tmp.hmdb, tmp.metab.name, dataset, tmp.datasets, features.to.include, tmp, contrib.hmdb, tmp.order, p1, all.plots, all.plots.2, i)
```


## SessionInfo

```{r}
sessionInfo()
```